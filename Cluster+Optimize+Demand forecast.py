# -*- coding: utf-8 -*-
"""Linh test 16

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16rCJHlzQ6Wn_51X78qSSDl36J6RLRqIR
"""

# --- B∆Ø·ªöC 0: IMPORT C√ÅC TH∆Ø VI·ªÜN C·∫¶N THI·∫æT ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from IPython.display import display # ƒê·ªÉ hi·ªÉn th·ªã DataFrame ƒë·∫πp h∆°n

# Th∆∞ vi·ªán Preprocessing
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA

# Th∆∞ vi·ªán M√¥ h√¨nh Clustering
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

# Th∆∞ vi·ªán ƒê√°nh gi√°
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Th∆∞ vi·ªán H·ªìi quy
import statsmodels.api as sm
import warnings

# C√†i ƒë·∫∑t hi·ªÉn th·ªã
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', '{:.2f}'.format)
warnings.filterwarnings('ignore')
print("-- [B∆Ø·ªöC 0] T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c nh·∫≠p th√†nh c√¥ng. --")


# --- B∆Ø·ªöC 1: T·∫¢I V√Ä K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG (FEATURE ENGINEERING) ---

print("\n-- [B∆Ø·ªöC 1] ƒêang t·∫£i d·ªØ li·ªáu v√† x√¢y d·ª±ng ƒë·∫∑c tr∆∞ng... --")
try:
    # T·∫£i T·∫§T C·∫¢ 4 t·ªáp c·∫ßn thi·∫øt ngay t·ª´ ƒë·∫ßu
    df_trans = pd.read_csv('transaction_data.csv')
    df_cust = pd.read_csv('customer_profile.csv')
    df_prod = pd.read_csv('product_master.csv')
    df_macro = pd.read_csv('macro_context.csv')

    # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu ng√†y th√°ng
    df_trans['Date_Time'] = pd.to_datetime(df_trans['Date_Time'])
    df_macro['Date'] = pd.to_datetime(df_macro['Date']).dt.date.astype(str)
    print("T·∫£i 4 t·ªáp d·ªØ li·ªáu th√†nh c√¥ng.")
except FileNotFoundError:
    print("L·ªñI: Kh√¥ng t√¨m th·∫•y 1 trong 4 t·ªáp CSV. Vui l√≤ng ƒë·∫£m b·∫£o 4 t·ªáp (trans, cust, prod, macro) n·∫±m trong c√πng th∆∞ m·ª•c.")
    # exit()

# 1.1. X√¢y d·ª±ng ƒë·∫∑c tr∆∞ng RFM
print("ƒêang x√¢y d·ª±ng ƒë·∫∑c tr∆∞ng RFM...")
snapshot_date = df_trans['Date_Time'].max() + pd.Timedelta(days=1)

rfm_df = df_trans.groupby('Customer_ID').agg(
    Recency=('Date_Time', lambda x: (snapshot_date - x.max()).days),
    Frequency=('Transaction_ID', 'nunique'),
    Monetary=('Total_Paid', 'sum')
).reset_index()

# 1.2. K·∫øt h·ª£p v√† X√¢y d·ª±ng ƒë·∫∑c tr∆∞ng H·ªì s∆° (Profile)
print("ƒêang k·∫øt h·ª£p d·ªØ li·ªáu v√† t·∫°o ƒë·∫∑c tr∆∞ng 'Age'...")
df_analysis = pd.merge(df_cust, rfm_df, on='Customer_ID', how='left')

# X·ª≠ l√Ω kh√°ch h√†ng kh√¥ng c√≥ giao d·ªãch (n·∫øu c√≥)
df_analysis['Recency'] = df_analysis['Recency'].fillna(999) # Gi·∫£ ƒë·ªãnh R=999 cho KH kh√¥ng c√≥ giao d·ªãch
df_analysis[['Frequency', 'Monetary']] = df_analysis[['Frequency', 'Monetary']].fillna(0)

# T·∫°o ƒë·∫∑c tr∆∞ng 'Age'
current_year = snapshot_date.year
df_analysis['Age'] = current_year - df_analysis['YoB']

# X·ª≠ l√Ω ngo·∫°i l·ªá (Outliers) cho RFM - S·ª≠ d·ª•ng Capping ·ªü Ph√¢n v·ªã 99%
print("ƒêang x·ª≠ l√Ω ngo·∫°i l·ªá cho RFM (capping ·ªü Q99)...")
for col in ['Recency', 'Frequency', 'Monetary']:
    cap_value = df_analysis[col].quantile(0.99)
    df_analysis[col] = df_analysis[col].clip(upper=cap_value)

# 1.3. Ch·ªçn c√°c ƒë·∫∑c tr∆∞ng cu·ªëi c√πng cho m√¥ h√¨nh
features_to_cluster = [
    'Age', 'Recency', 'Frequency', 'Monetary',  # S·ªë
    'Income level', 'Membership_Tier',          # X·∫øp h·∫°ng
    'Occupation', 'Gender'                      # Danh nghƒ©a
]
df_model_input = df_analysis[features_to_cluster].copy()
print(f"Ho√†n t·∫•t B∆Ø·ªöC 1. D·ªØ li·ªáu ƒë·∫ßu v√†o c√≥ {df_model_input.shape[0]} kh√°ch h√†ng v√† {df_model_input.shape[1]} ƒë·∫∑c tr∆∞ng.")


# --- B∆Ø·ªöC 2: TI·ªÄN X·ª¨ L√ù (PREPROCESSING PIPELINE) ---

print("\n-- [B∆Ø·ªöC 2] X√¢y d·ª±ng Pipeline M√£ h√≥a & Chu·∫©n h√≥a... --")

# 2.1. ƒê·ªãnh nghƒ©a c√°c nh√≥m c·ªôt
numerical_features = ['Age', 'Recency', 'Frequency', 'Monetary']
income_levels = ['< 2M', '2-5M', '5-10M', '10-20M', '20-50M', '> 50M']
membership_tiers = ['Standard', 'Silver', 'Gold', 'Diamond']
ordinal_features = ['Income level', 'Membership_Tier']
nominal_features = ['Occupation', 'Gender']

# 2.2. X√¢y d·ª±ng c√°c ƒë∆∞·ªùng ·ªëng (pipeline) con
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
ordinal_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OrdinalEncoder(categories=[income_levels, membership_tiers], handle_unknown='use_encoded_value', unknown_value=-1))
])
nominal_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# 2.3. K·∫øt h·ª£p c√°c pipeline con b·∫±ng ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('ord', ordinal_transformer, ordinal_features),
        ('nom', nominal_transformer, nominal_features)
    ],
    remainder='passthrough'
)

# 2.4. √Åp d·ª•ng preprocessor
X_scaled = preprocessor.fit_transform(df_model_input)
print("Pipeline ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t.")

# --- B∆Ø·ªöC 3: X√ÅC ƒê·ªäNH S·ªê C·ª§M T·ªêI ∆ØU (K) ---

print("\n-- [B∆Ø·ªöC 3] ƒêang x√°c ƒë·ªãnh K t·ªëi ∆∞u (Elbow & Silhouette)... --")
inertia_values = []
silhouette_scores = []
K_range = range(2, 9) # Ki·ªÉm tra t·ª´ 2 ƒë·∫øn 8 c·ª•m

print("ƒêang ch·∫°y K-Means v·ªõi c√°c K kh√°c nhau...")
for k in K_range:
    kmeans_test = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)
    kmeans_test.fit(X_scaled)
    inertia_values.append(kmeans_test.inertia_)
    score = silhouette_score(X_scaled, kmeans_test.labels_)
    silhouette_scores.append(score)

print("Ho√†n t·∫•t t√≠nh to√°n WCSS (Inertia) v√† Silhouette.")

# 3.1. V·∫Ω bi·ªÉu ƒë·ªì Elbow
plt.figure(figsize=(18, 6))
plt.subplot(1, 2, 1)
plt.plot(K_range, inertia_values, 'bo-'); plt.xlabel('S·ªë c·ª•m (K)'); plt.ylabel('WCSS (Inertia)'); plt.title('Ph∆∞∆°ng ph√°p Elbow (Elbow Method)'); plt.grid(True)
# 3.2. V·∫Ω bi·ªÉu ƒë·ªì Silhouette
plt.subplot(1, 2, 2)
plt.plot(K_range, silhouette_scores, 'rs-'); plt.xlabel('S·ªë c·ª•m (K)'); plt.ylabel('Silhouette Score'); plt.title('Ch·ªâ s·ªë Silhouette'); plt.grid(True)
plt.tight_layout()
plt.show()

# --- B∆Ø·ªöC 4: HU·∫§N LUY·ªÜN V√Ä ƒê√ÅNH GI√Å M√î H√åNH K-MEANS ---

print("\n-- [B∆Ø·ªöC 4] Hu·∫•n luy·ªán & ƒê√°nh gi√° m√¥ h√¨nh K-Means Clustering... --")
# Ch·ªçn K=3 d·ª±a tr√™n bi·ªÉu ƒë·ªì
N_CLUSTERS = 3

model_name = "K-Means"
model = KMeans(n_clusters=N_CLUSTERS, init='k-means++', random_state=42, n_init=10)
print(f"\nƒêang hu·∫•n luy·ªán m√¥ h√¨nh {model_name}...")
labels = model.fit_predict(X_scaled)
labels_dict = {model_name: labels} # L∆∞u nh√£n

silhouette = silhouette_score(X_scaled, labels)
davies_bouldin = davies_bouldin_score(X_scaled, labels)
print(f" - {model_name}: Silhouette Score = {silhouette:.4f}, Davies-Bouldin Index = {davies_bouldin:.4f}")
print("Ho√†n t·∫•t B∆Ø·ªöC 4.")


# --- B∆Ø·ªöC 5: GI·∫¢M CHI·ªÄU D·ªÆ LI·ªÜU B·∫∞NG PCA & TR·ª∞C QUAN H√ìA C·ª§M K-MEANS ---

print("\n-- [B∆Ø·ªöC 5] ƒêang gi·∫£m chi·ªÅu d·ªØ li·ªáu b·∫±ng PCA v√† Tr·ª±c quan h√≥a K-Means... --")
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)
df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
df_pca[model_name] = labels_dict[model_name]

# V·∫Ω bi·ªÉu ƒë·ªì PCA
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', hue=model_name, data=df_pca, palette='viridis', legend='full', alpha=0.7)
plt.title(f'{model_name} Clustering (PCA)')
plt.grid(True)
plt.tight_layout()
plt.show()
print("Ho√†n t·∫•t B∆Ø·ªöC 5.")

# --- B∆Ø·ªöC 6: PH√ÇN T√çCH H·ªí S∆† KH√ÅCH H√ÄNG THEO C·ª§M ---

print("\n-- [B∆Ø·ªöC 6] ƒêang ph√¢n t√≠ch h·ªì s∆° kh√°ch h√†ng theo c·ª•m... --")
df_analysis['Cluster'] = labels_dict["K-Means"]

print("\n--- H·ªì s∆° c·ª•m (ƒë·∫∑c tr∆∞ng s·ªë trung b√¨nh) ---")
cluster_profiles_num = df_analysis.groupby('Cluster')[numerical_features + ['Age']].mean()
display(cluster_profiles_num)

print("\n--- H·ªì s∆° c·ª•m (ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i ph·ªï bi·∫øn nh·∫•t) ---")
for col in ordinal_features + nominal_features:
    print(f"\nƒê·∫∑c tr∆∞ng: {col}")
    try:
        display(df_analysis.groupby('Cluster')[col].agg(lambda x: x.mode()[0] if not x.mode().empty else 'N/A').to_frame(name=f'Most_Frequent_{col}'))
    except IndexError:
        print(f"Kh√¥ng th·ªÉ t√≠nh mode cho c·ªôt {col}")
print("Ho√†n t·∫•t B∆Ø·ªöC 6.")

print("\n--- [GIAI ƒêO·∫†N 1] HO√ÄN T·∫§T. ---")
print("C√°c bi·∫øn df_trans, df_cust, df_prod, df_macro, v√† df_analysis (ƒë√£ c√≥ Cluster) ƒë√£ s·∫µn s√†ng.")

# --- [GIAI ƒêO·∫†N 2] T√çNH TO√ÅN ƒê·ªò CO GI√ÉN (PED) ---

# ##################################################################
# --- B∆Ø·ªöC 7 (M√¥ h√¨nh 5): T√çNH PED (LOGIC ƒê√öNG) ---
# Logic m·ªõi: log(Q_Category) ~ log(Price_Index_Category) + Controls
# ##################################################################

print("\n\n--- [B∆Ø·ªöC 7 - M√¥ h√¨nh 5] ƒêang t√≠nh PED (M√¥ h√¨nh Category + Price Index)... ---")

# 7.1. Chu·∫©n b·ªã d·ªØ li·ªáu (df_trans, df_prod, df_analysis ƒë√£ c√≥ trong b·ªô nh·ªõ)
print("ƒêang chu·∫©n b·ªã d·ªØ li·ªáu...")
try:
    # Merge trans + prod (ƒë·ªÉ l·∫•y Category)
    df_trans_with_cat = pd.merge(df_trans, df_prod[['Product_ID', 'Category']], on='Product_ID', how='left')

    # Merge (trans + prod) + analysis (ƒë·ªÉ l·∫•y Cluster)
    df_full_segmented = pd.merge(
        df_trans_with_cat,
        df_analysis[['Customer_ID', 'Cluster']],
        on='Customer_ID',
        how='left'
    )

    df_full_segmented = df_full_segmented.dropna(subset=['Cluster', 'Category'])
    df_full_segmented = df_full_segmented[df_full_segmented['Quantity'] > 0]
except NameError:
    print("L·ªñI: M·ªôt trong c√°c DataFrame (df_trans, df_prod, df_analysis) kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ch·∫°y file '1_Segmentation.py' tr∆∞·ªõc.")
    # exit()

# ƒê·∫£m b·∫£o c·ªôt 'Date' t·ªìn t·∫°i
df_full_segmented['Date'] = pd.to_datetime(df_full_segmented['Date_Time']).dt.date.astype(str)

# [LOGIC M·ªöI] T·∫°o c·ªôt Total_List_Price (T·ªïng gi√° ni√™m y·∫øt)
df_full_segmented['Total_List_Price'] = df_full_segmented['Quantity'] * df_full_segmented['Unit_Price_Listed']

# 7.2. T·ªïng h·ª£p theo Date, Cluster, V√Ä CATEGORY
print("ƒêang t·ªïng h·ª£p d·ªØ li·ªáu (L·∫•y T·ªîNG Q, T·ªîNG Paid, T·ªîNG List) theo Category...")
df_agg_cat = df_full_segmented.groupby(['Date', 'Cluster', 'Category']).agg(
    Total_Quantity=('Quantity', 'sum'),
    Total_Paid_Agg=('Total_Paid', 'sum'),
    Total_List_Price_Agg=('Total_List_Price', 'sum') # [LOGIC M·ªöI]
).reset_index()

# 7.3. [LOGIC M·ªöI] T·∫°o bi·∫øn "Price_Index" (cho t·ª´ng Category)
df_agg_cat = df_agg_cat[df_agg_cat['Total_List_Price_Agg'] > 0]
df_agg_cat['Price_Index'] = df_agg_cat['Total_Paid_Agg'] / df_agg_cat['Total_List_Price_Agg']
df_agg_cat = df_agg_cat[df_agg_cat['Price_Index'] > 0]
print("ƒê√£ t·∫°o 'Price_Index' (Total_Paid / Total_List_Price) cho t·ª´ng Category.")

# 7.4. Merge v·ªõi df_macro ƒë·ªÉ l·∫•y bi·∫øn ki·ªÉm so√°t
# (df_macro ƒë√£ ƒë∆∞·ª£c t·∫£i ·ªü B∆Ø·ªöC 1 v√† 'Date' ƒë√£ l√† string)
try:
    df_agg_cat['Date'] = df_agg_cat['Date'].astype(str) # ƒê·∫£m b·∫£o ki·ªÉu string

    df_agg_cat_macro = pd.merge(
        df_agg_cat,
        df_macro[['Date', 'Promotion_Campaign', 'Is_Weekend', 'Is_Holiday', 'Monthly_Index']],
        on='Date',
        how='left'
    )
    print("ƒê√£ th√™m c√°c bi·∫øn ki·ªÉm so√°t (Promotion, Weekend, Holiday, Monthly_Index).")

except Exception as e:
    print(f"L·ªñI NGHI√äM TR·ªåNG KHI MERGE: {e}")
    # exit()

# 7.5. Ph√¢n t√≠ch l·∫∑p qua t·ª´ng Cluster V√Ä Category
cluster_labels = sorted(df_analysis['Cluster'].unique())
categories_to_analyze = df_agg_cat_macro['Category'].unique()
ped_results_list_cat = []

print(f"B·∫Øt ƒë·∫ßu ch·∫°y h·ªìi quy (M√¥ h√¨nh 5) cho 3 C·ª•m x {len(categories_to_analyze)} Categories...")

for cluster in cluster_labels:
    for category in categories_to_analyze:

        df_cat_segment = df_agg_cat_macro[
            (df_agg_cat_macro['Cluster'] == cluster) &
            (df_agg_cat_macro['Category'] == category)
        ].copy()

        # 7.6. X√¢y d·ª±ng m√¥ h√¨nh Log-Log ƒêa bi·∫øn
        if df_cat_segment.shape[0] > 10: # C·∫ßn ƒë·ªß d·ªØ li·ªáu

            Y = np.log(df_cat_segment['Total_Quantity'])

            X = pd.DataFrame({
                'log_Price_Index': np.log(df_cat_segment['Price_Index']),
                'Promotion': df_cat_segment['Promotion_Campaign'],
                'Weekend': df_cat_segment['Is_Weekend'],
                'Holiday': df_cat_segment['Is_Holiday'],
                'Monthly_Index': df_cat_segment['Monthly_Index']
            })
            X = sm.add_constant(X)

            model_ols = sm.OLS(Y, X).fit()

            ped_value = model_ols.params['log_Price_Index']
            p_value = model_ols.pvalues['log_Price_Index'] # Gi·ªØ l·∫°i P-Value ƒë·ªÉ l·ªçc

            ped_results_list_cat.append({
                'Category': category,
                'Ph√¢n kh√∫c (Cluster)': cluster,
                'PED (Œ≤1)': ped_value,
                'P_Value': p_value
            })

        else:
            ped_results_list_cat.append({
                'Category': category,
                'Ph√¢n kh√∫c (Cluster)': cluster,
                'PED (Œ≤1)': np.nan,
                'P_Value': np.nan
            })

# 7.7. Hi·ªÉn th·ªã B·∫£ng k·∫øt qu·∫£ t·ªïng h·ª£p
print("\n--- [B∆Ø·ªöC 7 - M√¥ h√¨nh 5] Ho√†n t·∫•t t√≠nh to√°n PED. ---")
df_ped_summary_category = pd.DataFrame(ped_results_list_cat) # L∆∞u k·∫øt qu·∫£

# Hi·ªÉn th·ªã b·∫£ng l·ªçc |PED| > 1 (Theo y√™u c·∫ßu cu·ªëi c√πng c·ªßa b·∫°n)
print("\n--- B·∫£ng t√≥m t·∫Øt (Pivot) CH·ªà D√ÄNH CHO K·∫æT QU·∫¢ CO GI√ÉN (Elastic) ---")
# R√ÄNG BU·ªòC 2: L·ªçc 1 ƒëi·ªÅu ki·ªán duy nh·∫•t: Co gi√£n (Gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa PED > 1)
df_ped_elastic = df_ped_summary_category[
    (df_ped_summary_category['PED (Œ≤1)'].abs() > 1)
]

# ƒê·ªïi t√™n c·ª•m
cluster_name_map = { 0: 'C·ª•m 0', 1: 'C·ª•m 1', 2: 'C·ª•m 2' }
df_ped_elastic['Ph√¢n kh√∫c'] = df_ped_elastic['Ph√¢n kh√∫c (Cluster)'].map(cluster_name_map)

if df_ped_elastic.empty:
    print("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o Co gi√£n (|PED| > 1).")
else:
    df_pivot_elastic = df_ped_elastic.pivot_table(
        index='Category',
        columns='Ph√¢n kh√∫c',
        values='PED (Œ≤1)'
    )
    print(df_pivot_elastic.to_markdown(floatfmt=".3f"))
    print("\n(L∆∞u √Ω: Ch·ªâ hi·ªÉn th·ªã c√°c gi√° tr·ªã |PED| > 1, b·ªè qua P-Value)")

print("\n--- [GIAI ƒêO·∫†N 2] HO√ÄN T·∫§T. ---")
print("Bi·∫øn df_ped_elastic (ch·ªâ |PED| > 1) ƒë√£ s·∫µn s√†ng.")

# --- B∆Ø·ªöC 0: IMPORT C√ÅC TH∆Ø VI·ªÜN C·∫¶N THI·∫æT ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from IPython.display import display # ƒê·ªÉ hi·ªÉn th·ªã DataFrame ƒë·∫πp h∆°n

# Th∆∞ vi·ªán Preprocessing
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA

# Th∆∞ vi·ªán M√¥ h√¨nh Clustering
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture

# Th∆∞ vi·ªán ƒê√°nh gi√°
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Th∆∞ vi·ªán H·ªìi quy
import statsmodels.api as sm
import warnings

# [LOGIC M·ªöI] B·ªè th∆∞ vi·ªán Scipy Optimize
# import scipy.optimize as opt
# from scipy.optimize import minimize, NonlinearConstraint

# C√†i ƒë·∫∑t hi·ªÉn th·ªã
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', '{:.2f}'.format)
warnings.filterwarnings('ignore')
print("-- [B∆Ø·ªöC 0] T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c nh·∫≠p th√†nh c√¥ng. --")


# --- B∆Ø·ªöC 1: T·∫¢I V√Ä K·ª∏ THU·∫¨T ƒê·∫∂C TR∆ØNG (FEATURE ENGINEERING) ---

print("\n-- [B∆Ø·ªöC 1] ƒêang t·∫£i d·ªØ li·ªáu v√† x√¢y d·ª±ng ƒë·∫∑c tr∆∞ng... --")
try:
    # [S·ª¨A L·ªñI] T·∫£i T·∫§T C·∫¢ 4 t·ªáp c·∫ßn thi·∫øt ngay t·ª´ ƒë·∫ßu
    df_trans = pd.read_csv('transaction_data.csv')
    df_cust = pd.read_csv('customer_profile.csv')
    df_prod = pd.read_csv('product_master.csv')
    df_macro = pd.read_csv('macro_context.csv')

    # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu ng√†y th√°ng
    df_trans['Date_Time'] = pd.to_datetime(df_trans['Date_Time'])
    df_macro['Date'] = pd.to_datetime(df_macro['Date']).dt.date.astype(str)
    print("T·∫£i 4 t·ªáp d·ªØ li·ªáu th√†nh c√¥ng.")
except FileNotFoundError:
    print("L·ªñI: Kh√¥ng t√¨m th·∫•y 1 trong 4 t·ªáp CSV. Vui l√≤ng ƒë·∫£m b·∫£o 4 t·ªáp (trans, cust, prod, macro) n·∫±m trong c√πng th∆∞ m·ª•c.")
    # exit()

# 1.1. X√¢y d·ª±ng ƒë·∫∑c tr∆∞ng RFM
print("ƒêang x√¢y d·ª±ng ƒë·∫∑c tr∆∞ng RFM...")
snapshot_date = df_trans['Date_Time'].max() + pd.Timedelta(days=1)

rfm_df = df_trans.groupby('Customer_ID').agg(
    Recency=('Date_Time', lambda x: (snapshot_date - x.max()).days),
    Frequency=('Transaction_ID', 'nunique'),
    Monetary=('Total_Paid', 'sum')
).reset_index()

# 1.2. K·∫øt h·ª£p v√† X√¢y d·ª±ng ƒë·∫∑c tr∆∞ng H·ªì s∆° (Profile)
print("ƒêang k·∫øt h·ª£p d·ªØ li·ªáu v√† t·∫°o ƒë·∫∑c tr∆∞ng 'Age'...")
df_analysis = pd.merge(df_cust, rfm_df, on='Customer_ID', how='left')

# X·ª≠ l√Ω kh√°ch h√†ng kh√¥ng c√≥ giao d·ªãch (n·∫øu c√≥)
df_analysis['Recency'] = df_analysis['Recency'].fillna(999) # Gi·∫£ ƒë·ªãnh R=999 cho KH kh√¥ng c√≥ giao d·ªãch
df_analysis[['Frequency', 'Monetary']] = df_analysis[['Frequency', 'Monetary']].fillna(0)

# T·∫°o ƒë·∫∑c tr∆∞ng 'Age'
current_year = snapshot_date.year
df_analysis['Age'] = current_year - df_analysis['YoB']

# X·ª≠ l√Ω ngo·∫°i l·ªá (Outliers) cho RFM - S·ª≠ d·ª•ng Capping ·ªü Ph√¢n v·ªã 99%
print("ƒêang x·ª≠ l√Ω ngo·∫°i l·ªá cho RFM (capping ·ªü Q99)...")
for col in ['Recency', 'Frequency', 'Monetary']:
    cap_value = df_analysis[col].quantile(0.99)
    df_analysis[col] = df_analysis[col].clip(upper=cap_value)

# 1.3. Ch·ªçn c√°c ƒë·∫∑c tr∆∞ng cu·ªëi c√πng cho m√¥ h√¨nh
features_to_cluster = [
    'Age', 'Recency', 'Frequency', 'Monetary',  # S·ªë
    'Income level', 'Membership_Tier',          # X·∫øp h·∫°ng
    'Occupation', 'Gender'                      # Danh nghƒ©a
]
df_model_input = df_analysis[features_to_cluster].copy()
print(f"Ho√†n t·∫•t B∆Ø·ªöC 1. D·ªØ li·ªáu ƒë·∫ßu v√†o c√≥ {df_model_input.shape[0]} kh√°ch h√†ng v√† {df_model_input.shape[1]} ƒë·∫∑c tr∆∞ng.")


# --- B∆Ø·ªöC 2: TI·ªÄN X·ª¨ L√ù (PREPROCESSING PIPELINE) ---

print("\n-- [B∆Ø·ªöC 2] X√¢y d·ª±ng Pipeline M√£ h√≥a & Chu·∫©n h√≥a... --")

# 2.1. ƒê·ªãnh nghƒ©a c√°c nh√≥m c·ªôt
numerical_features = ['Age', 'Recency', 'Frequency', 'Monetary']
income_levels = ['< 2M', '2-5M', '5-10M', '10-20M', '20-50M', '> 50M']
membership_tiers = ['Standard', 'Silver', 'Gold', 'Diamond']
ordinal_features = ['Income level', 'Membership_Tier']
nominal_features = ['Occupation', 'Gender']

# 2.2. X√¢y d·ª±ng c√°c ƒë∆∞·ªùng ·ªëng (pipeline) con
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
ordinal_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OrdinalEncoder(categories=[income_levels, membership_tiers], handle_unknown='use_encoded_value', unknown_value=-1))
])
nominal_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# 2.3. K·∫øt h·ª£p c√°c pipeline con b·∫±ng ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('ord', ordinal_transformer, ordinal_features),
        ('nom', nominal_transformer, nominal_features)
    ],
    remainder='passthrough'
)

# 2.4. √Åp d·ª•ng preprocessor
X_scaled = preprocessor.fit_transform(df_model_input)
print("Pipeline ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t.")

# --- B∆Ø·ªöC 3: X√ÅC ƒê·ªäNH S·ªê C·ª§M T·ªêI ∆ØU (K) ---
# (B·ªè qua ph·∫ßn n√†y ƒë·ªÉ ch·∫°y nhanh h∆°n, gi·∫£ ƒë·ªãnh K=3)
print("\n-- [B∆Ø·ªöC 3] B·ªè qua (Gi·∫£ ƒë·ªãnh K=3)... --")

# --- B∆Ø·ªöC 4: HU·∫§N LUY·ªÜN V√Ä ƒê√ÅNH GI√Å M√î H√åNH K-MEANS ---

print("\n-- [B∆Ø·ªöC 4] Hu·∫•n luy·ªán & ƒê√°nh gi√° m√¥ h√¨nh K-Means Clustering... --")
N_CLUSTERS = 3
model_name = "K-Means"
model = KMeans(n_clusters=N_CLUSTERS, init='k-means++', random_state=42, n_init=10)
print(f"\nƒêang hu·∫•n luy·ªán m√¥ h√¨nh {model_name}...")
labels = model.fit_predict(X_scaled)
labels_dict = {model_name: labels} # L∆∞u nh√£n
print("Ho√†n t·∫•t B∆Ø·ªöC 4.")


# --- B∆Ø·ªöC 5: GI·∫¢M CHI·ªÄU D·ªÆ LI·ªÜU B·∫∞NG PCA & TR·ª∞C QUAN H√ìA C·ª§M K-MEANS ---
# (B·ªè qua ph·∫ßn n√†y ƒë·ªÉ ch·∫°y nhanh h∆°n)
print("\n-- [B∆Ø·ªöC 5] B·ªè qua (Kh√¥ng v·∫Ω bi·ªÉu ƒë·ªì PCA)... --")

# --- B∆Ø·ªöC 6: PH√ÇN T√çCH H·ªí S∆† KH√ÅCH H√ÄNG THEO C·ª§M ---

print("\n-- [B∆Ø·ªöC 6] ƒêang ph√¢n t√≠ch h·ªì s∆° kh√°ch h√†ng theo c·ª•m... --")
df_analysis['Cluster'] = labels_dict["K-Means"]
print("Ho√†n t·∫•t B∆Ø·ªöC 6.")


# ##################################################################
# --- B∆Ø·ªöC 7 (M√¥ h√¨nh 5): T√çNH PED (LOGIC ƒê√öNG) ---
# Logic m·ªõi: log(Q_Category) ~ log(Price_Index_Category) + Controls
# ##################################################################

print("\n\n--- [B∆Ø·ªöC 7 - M√¥ h√¨nh 5] ƒêang t√≠nh PED (M√¥ h√¨nh Category + Price Index)... ---")

# 7.1. Chu·∫©n b·ªã d·ªØ li·ªáu (df_trans, df_prod, df_analysis ƒë√£ c√≥ trong b·ªô nh·ªõ)
print("ƒêang chu·∫©n b·ªã d·ªØ li·ªáu...")
df_trans_with_cat = pd.merge(df_trans, df_prod[['Product_ID', 'Category']], on='Product_ID', how='left')
df_full_segmented = pd.merge(
    df_trans_with_cat,
    df_analysis[['Customer_ID', 'Cluster']],
    on='Customer_ID',
    how='left'
)
df_full_segmented = df_full_segmented.dropna(subset=['Cluster', 'Category'])
df_full_segmented = df_full_segmented[df_full_segmented['Quantity'] > 0]
df_full_segmented['Date'] = pd.to_datetime(df_full_segmented['Date_Time']).dt.date.astype(str)
df_full_segmented['Total_List_Price'] = df_full_segmented['Quantity'] * df_full_segmented['Unit_Price_Listed']

# Th√™m c·ªôt 'Effective_Price' v√†o df_full_segmented
df_full_segmented['Effective_Price'] = df_full_segmented['Total_Paid'] / df_full_segmented['Quantity']

# 7.2. T·ªïng h·ª£p theo Date, Cluster, V√Ä CATEGORY
print("ƒêang t·ªïng h·ª£p d·ªØ li·ªáu (L·∫•y T·ªîNG Q, T·ªîNG Paid, T·ªîNG List) theo Category...")
df_agg_cat = df_full_segmented.groupby(['Date', 'Cluster', 'Category']).agg(
    Total_Quantity=('Quantity', 'sum'),
    Total_Paid_Agg=('Total_Paid', 'sum'),
    Total_List_Price_Agg=('Total_List_Price', 'sum')
).reset_index()

# 7.3. [LOGIC M·ªöI] T·∫°o bi·∫øn "Price_Index" (cho t·ª´ng Category)
df_agg_cat = df_agg_cat[df_agg_cat['Total_List_Price_Agg'] > 0]
df_agg_cat['Price_Index'] = df_agg_cat['Total_Paid_Agg'] / df_agg_cat['Total_List_Price_Agg']
df_agg_cat = df_agg_cat[df_agg_cat['Price_Index'] > 0]
print("ƒê√£ t·∫°o 'Price_Index' (Total_Paid / Total_List_Price) cho t·ª´ng Category.")

# 7.4. Merge v·ªõi df_macro ƒë·ªÉ l·∫•y bi·∫øn ki·ªÉm so√°t
df_agg_cat['Date'] = df_agg_cat['Date'].astype(str)
df_agg_cat_macro = pd.merge(
    df_agg_cat,
    df_macro[['Date', 'Promotion_Campaign', 'Is_Weekend', 'Is_Holiday', 'Monthly_Index']],
    on='Date',
    how='left'
)
print("ƒê√£ th√™m c√°c bi·∫øn ki·ªÉm so√°t (Promotion, Weekend, Holiday, Monthly_Index).")

# 7.5. Ph√¢n t√≠ch l·∫∑p qua t·ª´ng Cluster V√Ä Category
cluster_labels = sorted(df_analysis['Cluster'].unique())
categories_to_analyze = df_agg_cat_macro['Category'].unique()
ped_results_list_cat = []

print(f"B·∫Øt ƒë·∫ßu ch·∫°y h·ªìi quy (M√¥ h√¨nh 5) cho 3 C·ª•m x {len(categories_to_analyze)} Categories...")

for cluster in cluster_labels:
    for category in categories_to_analyze:

        df_cat_segment = df_agg_cat_macro[
            (df_agg_cat_macro['Cluster'] == cluster) &
            (df_agg_cat_macro['Category'] == category)
        ].copy()

        # 7.6. X√¢y d·ª±ng m√¥ h√¨nh Log-Log ƒêa bi·∫øn
        if df_cat_segment.shape[0] > 10: # C·∫ßn ƒë·ªß d·ªØ li·ªáu

            Y = np.log(df_cat_segment['Total_Quantity'])

            X = pd.DataFrame({
                'log_Price_Index': np.log(df_cat_segment['Price_Index']),
                'Promotion': df_cat_segment['Promotion_Campaign'],
                'Weekend': df_cat_segment['Is_Weekend'],
                'Holiday': df_cat_segment['Is_Holiday'],
                'Monthly_Index': df_cat_segment['Monthly_Index']
            })
            X = sm.add_constant(X)

            model_ols = sm.OLS(Y, X).fit()

            ped_value = model_ols.params['log_Price_Index']
            p_value = model_ols.pvalues['log_Price_Index'] # [S·ª¨A L·ªñI] Gi·ªØ l·∫°i P-Value

            ped_results_list_cat.append({
                'Category': category,
                'Ph√¢n kh√∫c (Cluster)': cluster,
                'PED (Œ≤1)': ped_value,
                'P_Value': p_value # [S·ª¨A L·ªñI] Gi·ªØ l·∫°i P-Value
            })

        else:
            ped_results_list_cat.append({
                'Category': category,
                'Ph√¢n kh√∫c (Cluster)': cluster,
                'PED (Œ≤1)': np.nan,
                'P_Value': np.nan # [S·ª¨A L·ªñI] Gi·ªØ l·∫°i P-Value
            })

# 7.7. Hi·ªÉn th·ªã B·∫£ng k·∫øt qu·∫£ t·ªïng h·ª£p
print("\n--- [B∆Ø·ªöC 7 - M√¥ h√¨nh 5] Ho√†n t·∫•t t√≠nh to√°n PED. ---")
df_ped_summary_category = pd.DataFrame(ped_results_list_cat) # L∆∞u k·∫øt qu·∫£

# Hi·ªÉn th·ªã b·∫£ng l·ªçc |PED| > 1 (Theo y√™u c·∫ßu cu·ªëi c√πng c·ªßa b·∫°n)
print("\n--- B·∫£ng t√≥m t·∫Øt (Pivot) CH·ªà D√ÄNH CHO K·∫æT QU·∫¢ CO GI√ÉN (Elastic) ---")
# R√ÄNG BU·ªòC 2: L·ªçc 1 ƒëi·ªÅu ki·ªán duy nh·∫•t: Co gi√£n (Gi√° tr·ªã tuy·ªát ƒë·ªëi c·ªßa PED > 1)
df_ped_elastic = df_ped_summary_category[
    (df_ped_summary_category['PED (Œ≤1)'].abs() > 1)
]

# ƒê·ªïi t√™n c·ª•m
cluster_name_map = { 0: 'C·ª•m 0', 1: 'C·ª•m 1', 2: 'C·ª•m 2' }
df_ped_elastic['Ph√¢n kh√∫c'] = df_ped_elastic['Ph√¢n kh√∫c (Cluster)'].map(cluster_name_map)

if df_ped_elastic.empty:
    print("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o Co gi√£n (|PED| > 1).")
else:
    df_pivot_elastic = df_ped_elastic.pivot_table(
        index='Category',
        columns='Ph√¢n kh√∫c',
        values='PED (Œ≤1)'
    )
    print(df_pivot_elastic.to_markdown(floatfmt=".3f"))
    print("\n(L∆∞u √Ω: Ch·ªâ hi·ªÉn th·ªã c√°c gi√° tr·ªã |PED| > 1, b·ªè qua P-Value)")


# ##################################################################
# --- B∆Ø·ªöC 8-11: T·ªêI ∆ØU H√ìA (OPTIMIZATION) ---
# [LOGIC M·ªöI] Quay l·∫°i Grid Search + 7 R√†ng bu·ªôc
# ##################################################################

print("\n\n--- [B∆Ø·ªöC 8-10] ƒêang ƒë·ªãnh nghƒ©a H√†m T·ªëi ∆∞u h√≥a (Grid Search)... ---")

# R√ÄNG BU·ªòC 6: B·∫£o v·ªá Th·ªã ph·∫ßn (Kh√¥ng gi·∫£m qu√° 15%)
MAX_QUANTITY_DROP_PCT = 0.15
print(f"    [R√†ng bu·ªôc 6] R√†ng bu·ªôc s·ª•t gi·∫£m s·∫£n l∆∞·ª£ng t·ªëi ƒëa: {MAX_QUANTITY_DROP_PCT:.0%}")

# [LOGIC M·ªöI] R√ÄNG BU·ªòC 7: "Business Sense" - Gi·ªõi h·∫°n tƒÉng gi√° t·ªëi ƒëa
MAX_PRICE_INCREASE_PCT = 0.20 # TƒÉng t·ªëi ƒëa 20%
print(f"    [R√†ng bu·ªôc 7] R√†ng bu·ªôc tƒÉng gi√° t·ªëi ƒëa: {MAX_PRICE_INCREASE_PCT:.0%}")


# [B∆Ø·ªöC 8] H√†m chu·∫©n b·ªã d·ªØ li·ªáu
def prepare_base_data_optimization():
    print("    [B8] ƒêang chu·∫©n b·ªã D·ªØ li·ªáu N·ªÅn (P_base, Q_base, COGS, PED)...")
    try:
        # 1. P_base (Gi√° b√°n th·ª±c t·∫ø TB) cho m·ªói SKU
        # (df_full_segmented ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü B∆Ø·ªöC 7)
        df_p_base = df_full_segmented.groupby('Product_ID')['Effective_Price'].mean().to_frame('P_base')

        # 2. Q_base (T·ªïng s·∫£n l∆∞·ª£ng c∆° s·ªü) cho m·ªói SKU v√† t·ª´ng C·ª•m
        df_q_total_base = df_full_segmented.groupby('Product_ID')['Quantity'].sum().to_frame('Q_total_base')
        df_q_base_pivot = df_full_segmented.groupby(['Product_ID', 'Cluster'])['Quantity'].sum().unstack(fill_value=0)
        df_q_base_pivot.columns = [f'Q_base_{col}' for col in df_q_base_pivot.columns]

        # 3. L·∫•y COGS c≈© (C·∫ßn df_prod)
        df_cogs = df_prod.set_index('Product_ID')[['COGS', 'Category']]

        # 4. [LOGIC M·ªöI] √Ånh x·∫° PED (t·ª´ B∆Ø·ªöC 7) theo Category
        # (df_ped_elastic ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü B∆Ø·ªöC 7)
        print("    [B8] ƒêang √°nh x·∫° PED t·ª´ B∆Ø·ªöC 7 (ch·ªâ l·∫•y |PED| > 1)...")

        # R√ÄNG BU·ªòC L·ªåC: Ch·ªâ l·∫•y PED c√≥ |PED| > 1

        # Pivot c√°c PED "Elastic"
        df_ped_pivot = df_ped_elastic.pivot_table(
            index='Category',
            columns='Ph√¢n kh√∫c (Cluster)', # D√πng Cluster (s·ªë)
            values='PED (Œ≤1)'
        )
        df_ped_pivot.columns = [f'PED_{col}' for col in df_ped_pivot.columns]

        # G·∫Øn PED v√†o b·∫£ng COGS (theo Category)
        # [S·ª¨A L·ªñI] S·ª≠a l·∫°i logic merge: D√πng left_on='Category' v√† right_index=True
        df_cogs_with_ped = pd.merge(df_cogs, df_ped_pivot, left_on='Category', right_index=True, how='left')

        # ##########################################################
        # [LOGIC M·ªöI] R√ÄNG BU·ªòC 3b: (Business Sense)
        # G√°n PED = -1.0 (Unit Elastic) cho c√°c ph√¢n kh√∫c "Inelastic" (NaN)
        # ##########################################################
        ped_cols = ['PED_0', 'PED_1', 'PED_2']
        df_cogs_with_ped[ped_cols] = df_cogs_with_ped[ped_cols].fillna(-1.0)
        print("          -> ƒê√£ g√°n PED = -1.0 (Unit Elastic) cho c√°c ph√¢n kh√∫c 'Inelastic' (NaN).")

        # 5. H·ª£p nh·∫•t t·∫•t c·∫£
        df_base = pd.concat([
            df_p_base, df_q_total_base, df_q_base_pivot, df_cogs_with_ped
        ], axis=1)

        df_base = df_base.dropna(subset=['P_base', 'Q_total_base'])
        print(f"    [B8] ƒê√£ chu·∫©n b·ªã xong D·ªØ li·ªáu N·ªÅn cho {len(df_base)} SKUs.")
        return df_base.copy()

    except NameError as e:
        print(f"    [B8] L·ªñI NameError: {e}. Vui l√≤ng ƒë·∫£m b·∫£o B∆Ø·ªöC 1-7 (ƒë·∫∑c bi·ªát l√† df_trans, df_analysis, df_prod, df_ped_summary_category) ƒë√£ ch·∫°y.")
        return None
    except Exception as e:
        print(f"    [B8] L·ªñI: {e}")
        return None

# [B∆Ø·ªöC 10] H√†m t√≠nh to√°n cho 1 SKU (Logic 1 Gi√°, Grid Search)
def optimize_sku_gridsearch(row):
    # L·∫•y d·ªØ li·ªáu n·ªÅn
    p_base = row['P_base']
    q_total_base = row['Q_total_base']
    cogs_new = row['COGS_new']

    clusters = [0, 1, 2]
    q_base = {}
    ped = {}

    # R√ÄNG BU·ªòC 3: G√°n Th·ªã ph·∫ßn v√† PED
    for c in clusters:
        q_base[c] = row.get(f'Q_base_{c}', 0)
        ped[c] = row.get(f'PED_{c}', -1.0) # L·∫•y PED (N·∫øu l·ªói, m·∫∑c ƒë·ªãnh -1.0)

    profit_at_p_base = 0.0
    q_at_p_base = 0.0

    # 1. T√≠nh Profit_at_P_base (v·ªõi COGS_new)
    for c in clusters:
        profit_at_p_base += (p_base - cogs_new) * q_base[c]
        q_at_p_base += q_base[c]

    # 2. ƒê·ªãnh nghƒ©a R√†ng bu·ªôc
    # R√ÄNG BU·ªòC 4 (Chi ph√≠): P >= COGS_new
    p_min = max(p_base, cogs_new)

    # R√ÄNG BU·ªòC 7 ("Business Sense"): TƒÉng gi√° t·ªëi ƒëa 20%
    p_max = p_base * (1 + MAX_PRICE_INCREASE_PCT)

    # R√ÄNG BU·ªòC 6 (Th·ªã ph·∫ßn):
    q_min_allowed = q_at_p_base * (1 - MAX_QUANTITY_DROP_PCT)

    if p_min > p_max:
        return pd.Series({'P_optimal': p_base, 'Profit_optimal': profit_at_p_base, 'Q_optimal': q_at_p_base, 'Profit_at_P_base': profit_at_p_base, 'Status': f'L·ªói: COGS_new ({cogs_new:,.0f}) > P_max ({p_max:,.0f})'})

    price_grid = np.linspace(p_min, p_max, 100) # Qu√©t 100 ƒëi·ªÉm

    best_profit = profit_at_p_base
    best_p = p_base
    best_q = q_at_p_base

    # 3. Ch·∫°y Grid Search (T√¨m Profit Max)
    for p_new in price_grid:
        # B·∫Øt ƒë·∫ßu t·ª´ p_min (ƒë√£ bao g·ªìm p_base)

        total_profit_new = 0
        total_q_new = 0

        # R√ÄNG BU·ªòC 1, 2, 5 (H√†m c·∫ßu, PED, 1 Gi√°)
        for c in clusters:
            if q_base[c] == 0: continue
            # C√¥ng th·ª©c Q_new
            q_new_c = q_base[c] * (p_new / p_base) ** ped[c]
            # C√¥ng th·ª©c Profit_new
            profit_new_c = (p_new - cogs_new) * q_new_c

            total_profit_new += profit_new_c
            total_q_new += q_new_c

        # R√ÄNG BU·ªòC 6 (Th·ªã ph·∫ßn)
        if total_q_new < q_min_allowed:
            continue # Vi ph·∫°m, b·ªè qua m·ª©c gi√° n√†y

        # H√ÄM M·ª§C TI√äU: Maximize Profit
        # [S·ª¨A L·ªñI LOGIC] Ph·∫£i d√πng >= ƒë·ªÉ ghi ƒë√® gi√° tr·ªã p_base
        if total_profit_new >= best_profit:
            best_profit, best_p, best_q = total_profit_new, p_new, total_q_new

    return pd.Series({'P_optimal': best_p, 'Profit_optimal': best_profit, 'Q_optimal': best_q, 'Profit_at_P_base': profit_at_p_base, 'Status': 'Th√†nh c√¥ng'})

# [B∆Ø·ªöC 9 & 11] H√†m ch·∫°y ch√≠nh
def run_optimization(cogs_input_dict, df_base_data):
    if df_base_data is None:
        print("    [B11] L·ªñI: D·ªØ li·ªáu n·ªÅn (df_base_data) r·ªóng.")
        return

    df_base = df_base_data.copy()
    df_base['COGS_new'] = df_base['COGS']
    if cogs_input_dict:
        cogs_new_series = pd.Series(cogs_input_dict)
        cogs_new_series.name = 'COGS_new_override'
        df_base = df_base.merge(cogs_new_series, left_index=True, right_index=True, how='left')
        df_base['COGS_new'] = df_base['COGS_new_override'].fillna(df_base['COGS_new'])
        df_base = df_base.drop(columns=['COGS_new_override'])
        print(f"    [B9] ƒê√£ √°p d·ª•ng COGS_new cho {len(cogs_input_dict)} SKUs.")
    else:
        print("    [B9] Kh√¥ng c√≥ COGS_new n√†o ƒë∆∞·ª£c nh·∫≠p. D√πng COGS hi·ªán t·∫°i.")

    print(f"    [B11] ƒêang ch·∫°y T·ªëi ∆∞u h√≥a (Grid Search) cho {len(df_base)} SKUs...")
    results_list = df_base.apply(optimize_sku_gridsearch, axis=1) # G·ªçi h√†m Grid Search

    df_final_results = df_base.join(results_list)

    # T√≠nh to√°n c√°c ch·ªâ s·ªë c·∫£i thi·ªán
    df_final_results['Profit_Increase'] = df_final_results['Profit_optimal'] - df_final_results['Profit_at_P_base']
    df_final_results['Profit_Increase_Pct'] = (df_final_results['Profit_Increase'] / df_final_results['Profit_at_P_base']).replace([np.inf, -np.inf], np.nan)
    df_final_results['Q_Drop_Pct'] = (df_final_results['Q_total_base'] - df_final_results['Q_optimal']) / df_final_results['Q_total_base']
    df_final_results['P_Increase_Pct'] = (df_final_results['P_optimal'] - df_final_results['P_base']) / df_final_results['P_base']

    df_final_results = df_final_results.sort_values(by='Profit_Increase', ascending=False)

    columns_to_show = [
        'COGS_new', 'P_base', 'P_optimal', 'Profit_at_P_base', 'Profit_optimal',
        'Profit_Increase', 'Profit_Increase_Pct', 'Q_total_base', 'Q_optimal',
        'Q_Drop_Pct', 'P_Increase_Pct', 'Status'
    ]
    ped_cols_to_show = ['Category', 'PED_0', 'PED_1', 'PED_2']

    print("\n--- K·∫æT QU·∫¢ T·ªêI ∆ØU H√ìA GI√Å (ƒê√É S·∫ÆP X·∫æP) ---")
    print(f"(R√†ng bu·ªôc: TƒÉng gi√° t·ªëi ƒëa {MAX_PRICE_INCREASE_PCT:.0%}, S·ª•t gi·∫£m S·∫£n l∆∞·ª£ng t·ªëi ƒëa {MAX_QUANTITY_DROP_PCT:.0%})")

    # [S·ª¨A L·ªñI] S·ª≠a l·ªói th·ª•t l·ªÅ
    display(df_final_results[columns_to_show].style.format({
        'COGS_new': '{:,.0f}', 'P_base': '{:,.0f}', 'P_optimal': '{:,.0f}',
        'Profit_at_P_base': '{:,.0f}', 'Profit_optimal': '{:,.0f}', 'Profit_Increase': '{:,.0f}',
        'Q_total_base': '{:,.0f}', 'Q_optimal': '{:,.0f}',
        'Profit_Increase_Pct': '{:.1%}',
        'Q_Drop_Pct': '{:.1%}',
        'P_Increase_Pct': '{:.1%}'
    }))

    print("\n--- B·∫£ng tra c·ª©u PED ƒë√£ s·ª≠ d·ª•ng (-1.0 = Gi·∫£ ƒë·ªãnh Inelastic) ---")
    # [S·ª¨A L·ªñI] S·ª≠a l·ªói format
    ped_format_dict = {
        'PED_0': '{:.3f}',
        'PED_1': '{:.3f}',
        'PED_2': '{:.3f}'
    }
    display(df_final_results[ped_cols_to_show].style.format(ped_format_dict))

# --- CH·∫†Y KH·ªêI 1 (B∆Ø·ªöC 0-7) ---
print("--- [B·∫ÆT ƒê·∫¶U] Ch·∫°y B∆Ø·ªöC 0-7 (Ph√¢n kh√∫c v√† T√≠nh PED)... ---")
# (To√†n b·ªô code B∆Ø·ªöC 0-7 ch·∫°y ·ªü tr√™n)
print("\n--- [HO√ÄN T·∫§T] B∆Ø·ªöC 0-7. ---")


# --- CH·∫†Y KH·ªêI 2 (B∆Ø·ªöC 8-11) ---
print("\n--- [B·∫ÆT ƒê·∫¶U] Ch·∫°y B∆Ø·ªöC 8-11 (T·ªëi ∆∞u h√≥a)... ---")
GLOBAL_DF_BASE_DATA = prepare_base_data_optimization()

if GLOBAL_DF_BASE_DATA is not None:
    # ##################################################################
    # B·∫¢NG ƒêI·ªÄU KHI·ªÇN INPUT C·ª¶A B·∫†N
    # ##################################################################
    cogs_input_cua_ban = {
        # === Coffee (C≈© + 6,000) ===
        'CF01_S': 16500,  # (C≈©: 10500)
        'CF01_M': 19500,  # (C≈©: 13500)
        'CF01_L': 22500,  # (C≈©: 16500)
        'CF02_S': 16500,  # (C≈©: 10500)
        'CF02_M': 19500,  # (C≈©: 13500)
        'CF02_L': 22500,  # (C≈©: 16500)
        'CF03_S': 16500,  # (C≈©: 10500)
        'CF03_M': 19500,  # (C≈©: 13500)
        'CF03_L': 22500,  # (C≈©: 16500)

        # === Tea (C≈© + 6,000) ===
        'TE04_S': 18250,  # (C≈©: 12250)
        'TE04_M': 20750,  # (C≈©: 14750)
        'TE04_L': 23250,  # (C≈©: 17250)
        'TE05_S': 18250,  # (C≈©: 12250)
        'TE05_M': 20750,  # (C≈©: 14750)
        'TE05_L': 23250,  # (C≈©: 17250)
        'TE06_S': 18250,  # (C≈©: 12250)
        'TE06_M': 20750,  # (C≈©: 14750)
        'TE06_L': 23250,  # (C≈©: 17250)

        # === Freeze (C≈© + 6,000) ===
        'FR07_S': 23250,  # (C≈©: 17250)
        'FR07_M': 26250,  # (C≈©: 20250)
        'FR07_L': 29250,  # (C≈©: 23250)
        'FR08_S': 23250,  # (C≈©: 17250)
        'FR08_M': 26250,  # (C≈©: 20250)
        'FR08_L': 29250,  # (C≈©: 23250)
        'FR09_S': 23250,  # (C≈©: 17250)
        'FR09_M': 26250,  # (C≈©: 20250)
        'FR09_L': 29250,  # (C≈©: 23250)
    }
    # ##################################################################

    print(f"\n--- [KH·ªêI 2] B·∫Øt ƒë·∫ßu ch·∫°y T·ªëi ∆∞u h√≥a (Grid Search) cho {len(cogs_input_cua_ban)} SKU... ---")
    run_optimization(cogs_input_cua_ban, GLOBAL_DF_BASE_DATA)
else:
    print("\nL·ªñI: Kh√¥ng th·ªÉ ch·∫°y T·ªëi ∆∞u h√≥a v√¨ D·ªØ li·ªáu N·ªÅn (B∆Ø·ªöC 8) ƒë√£ th·∫•t b·∫°i.")

print("\n--- [TO√ÄN B·ªò QUY TR√åNH] HO√ÄN T·∫§T. ---")

pip install cmdstanpy

# --- [FILE STANDALONE]: D·ª∞ B√ÅO Q1/2025 V·ªöI 4 M√î H√åNH (ƒê√É S·ª¨A L·ªñI HI·ªÇN TH·ªä) ---
# M·ª•c ti√™u:
# 1. D√πng d·ªØ li·ªáu 2024 ƒë·ªÉ hu·∫•n luy·ªán 4 m√¥ h√¨nh d·ª± b√°o L∆∞·ª£ng b√°n (Quantity)
#    d·ª±a tr√™n c√°c y·∫øu t·ªë nh∆∞ Gi√°, L·ªÖ, Cu·ªëi tu·∫ßn.
# 2. T·∫°o d·ªØ li·ªáu 90 ng√†y cho Q1/2025.
# 3. D·ª± b√°o Q1/2025 v·ªõi 2 k·ªãch b·∫£n gi√°.
# 4. T√≠nh to√°n Doanh thu & L·ª£i nhu·∫≠n cho t·ª´ng k·ªãch b·∫£n.
# -----------------------------------------------------------------

# --- B∆Ø·ªöC 0: IMPORT TH∆Ø VI·ªÜN ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import timedelta
import warnings
from IPython.display import display, Markdown

# Th∆∞ vi·ªán Preprocessing
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Th∆∞ vi·ªán 4 M√¥ h√¨nh
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
# from prophet import Prophet # ƒê√É B·ªé DO L·ªñI K·ª∏ THU·∫¨T
# T·∫Øt c·∫£nh b√°o TensorFlow
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
tf.get_logger().setLevel('ERROR')
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# Th∆∞ vi·ªán ƒê√°nh gi√° (ƒë·ªÉ tham kh·∫£o)
from sklearn.metrics import mean_absolute_error

# C√†i ƒë·∫∑t
pd.set_option('display.float_format', '{:.2f}'.format)
warnings.filterwarnings('ignore')
print("-- [B∆Ø·ªöC 0] T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c nh·∫≠p th√†nh c√¥ng. --")


# --- [GI·∫¢ ƒê·ªäNH K·ªäCH B·∫¢N] ---
# ƒê√¢y l√† c√°c gi·∫£ ƒë·ªãnh t·ª´ file T·ªëi ∆∞u h√≥a (Optimization)

# K·ªãch b·∫£n A: Gi·ªØ nguy√™n gi√°
PRICE_INDEX_A = 0.95

# K·ªãch b·∫£n B: TƒÉng gi√° theo T·ªëi ∆∞u h√≥a
PRICE_INDEX_B = 1.02

# Gi·∫£ ƒë·ªãnh v·ªÅ Gi√° tr·ªã & Chi ph√≠ (ƒë·ªÉ t√≠nh l·ª£i nhu·∫≠n)
AVG_LIST_PRICE_PER_ITEM = 55000 # Gi√° ni√™m y·∫øt trung b√¨nh
NEW_COGS_PER_ITEM = 18000 # Gi√° v·ªën m·ªõi (sau khi tƒÉng 6k)

# T√≠nh to√°n gi√° b√°n th·ª±c t·∫ø cho m·ªói k·ªãch b·∫£n
PRICE_PAID_A = AVG_LIST_PRICE_PER_ITEM * PRICE_INDEX_A
PRICE_PAID_B = AVG_LIST_PRICE_PER_ITEM * PRICE_INDEX_B
print(f"-- [GI·∫¢ ƒê·ªäNH] K·ªãch b·∫£n A (Gi·ªØ gi√°): {PRICE_PAID_A:,.0f} ƒë / 1 ly")
print(f"-- [GI·∫¢ ƒê·ªäNH] K·ªãch b·∫£n B (TƒÉng gi√°): {PRICE_PAID_B:,.0f} ƒë / 1 ly")
print(f"-- [GI·∫¢ ƒê·ªäNH] Gi√° v·ªën m·ªõi: {NEW_COGS_PER_ITEM:,.0f} ƒë / 1 ly")
# ----------------------------------


# --- B∆Ø·ªöC 1: T·∫¢I V√Ä T·ªîNG H·ª¢P D·ªÆ LI·ªÜU (T·∫†O B·∫¢NG HU·∫§N LUY·ªÜN 2024) ---
print("\n-- [B∆Ø·ªöC 1] ƒêang t·∫£i v√† t·∫°o B·∫£ng d·ªØ li·ªáu hu·∫•n luy·ªán (2024)... --")
try:
    df_trans = pd.read_csv('transaction_data.csv')
    df_prod = pd.read_csv('product_master.csv') # C·∫ßn ƒë·ªÉ l·∫•y gi√° ni√™m y·∫øt
    df_macro = pd.read_csv('macro_context.csv')
except FileNotFoundError:
    print("L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp CSV. Vui l√≤ng ƒë·∫£m b·∫£o 3 t·ªáp (trans, prod, macro) n·∫±m trong c√πng th∆∞ m·ª•c.")
    # exit()

# Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
df_trans['Date_Time'] = pd.to_datetime(df_trans['Date_Time'])
df_trans['Date'] = df_trans['Date_Time'].dt.date.astype(str)
df_macro['Date'] = pd.to_datetime(df_macro['Date']).dt.date.astype(str)

# 1.1. T√≠nh to√°n 'Total_List_Price' cho t·ª´ng giao d·ªãch
df_trans = pd.merge(
    df_trans,
    df_prod[['Product_ID', 'Unit_Price_List']],
    on='Product_ID',
    how='left'
)
df_trans = df_trans.rename(columns={'Unit_Price_Listed': 'Unit_Price_Recorded', 'Unit_Price_List': 'Unit_Price_Master'})
df_trans['Unit_Price_Master'] = df_trans['Unit_Price_Master'].fillna(df_trans['Unit_Price_Recorded'])
df_trans['Total_List_Price'] = df_trans['Quantity'] * df_trans['Unit_Price_Master']


# 1.2. T·ªïng h·ª£p (Aggregate) to√†n b·ªô gi·ªè h√†ng l√™n c·∫•p ƒë·ªô H√ÄNG NG√ÄY
df_daily_agg = df_trans.groupby('Date').agg(
    Total_Quantity=('Quantity', 'sum'),
    Total_Paid_Agg=('Total_Paid', 'sum'),
    Total_List_Price_Agg=('Total_List_Price', 'sum')
).reset_index()

# 1.3. T√≠nh to√°n 'Price_Index' (BI·∫æN NGUY√äN NH√ÇN CH√çNH)
df_daily_agg = df_daily_agg[df_daily_agg['Total_List_Price_Agg'] > 0]
df_daily_agg['Price_Index'] = df_daily_agg['Total_Paid_Agg'] / df_daily_agg['Total_List_Price_Agg']

# 1.4. G·∫Øn c√°c bi·∫øn ngo·∫°i lai (Macro)
df_train_2024 = pd.merge(
    df_daily_agg,
    df_macro[['Date', 'Is_Weekend', 'Is_Holiday', 'Promotion_Campaign']],
    on='Date',
    how='left'
)

# 1.5. T·∫°o c√°c ƒë·∫∑c tr∆∞ng th·ªùi gian
df_train_2024['Date'] = pd.to_datetime(df_train_2024['Date'])
df_train_2024 = df_train_2024.set_index('Date')
df_train_2024['Month'] = df_train_2024.index.month
df_train_2024['DayOfWeek'] = df_train_2024.index.dayofweek

# 1.6. Final check
df_train_2024 = df_train_2024.dropna()
print(f"Ho√†n t·∫•t B∆Ø·ªöC 1. ƒê√£ t·∫°o b·∫£ng hu·∫•n luy·ªán 2024 v·ªõi {len(df_train_2024)} ng√†y.")
display(df_train_2024.head())


# --- B∆Ø·ªöC 2: T·∫†O D·ªÆ LI·ªÜU T∆Ø∆†NG LAI (Q1/2025) ---
print("\n-- [B∆Ø·ªöC 2] ƒêang t·∫°o B·∫£ng d·ªØ li·ªáu t∆∞∆°ng lai (Q1/2025)... --")
N_DAYS_FORECAST = 90
last_date_2024 = df_train_2024.index.max()
future_dates = pd.date_range(start=last_date_2024 + timedelta(days=1), periods=N_DAYS_FORECAST, freq='D')

df_future_2025 = pd.DataFrame(index=future_dates)
df_future_2025['Is_Weekend'] = (df_future_2025.index.dayofweek >= 5).astype(int)
df_future_2025['Month'] = df_future_2025.index.month
df_future_2025['DayOfWeek'] = df_future_2025.index.dayofweek

# Gi·∫£ ƒë·ªãnh v·ªÅ t∆∞∆°ng lai:
df_future_2025['Is_Holiday'] = 0
df_future_2025['Promotion_Campaign'] = 0

print(f"Ho√†n t·∫•t B∆Ø·ªöC 2. ƒê√£ t·∫°o b·∫£ng t∆∞∆°ng lai {N_DAYS_FORECAST} ng√†y.")
display(df_future_2025.head())


# --- B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN V√Ä D·ª∞ B√ÅO V·ªöI 4 M√î H√åNH ---
print("\n-- [B∆Ø·ªöC 3] B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán v√† d·ª± b√°o v·ªõi 4 m√¥ h√¨nh... --")

# 3.1. ƒê·ªãnh nghƒ©a Bi·∫øn Y (Target) v√† X (Features)
target_col = 'Total_Quantity'
feature_cols = ['Price_Index', 'Is_Weekend', 'Is_Holiday', 'Promotion_Campaign', 'Month', 'DayOfWeek']

Y_train = df_train_2024[target_col]
X_train = df_train_2024[feature_cols]

# 3.2. T·∫°o 2 k·ªãch b·∫£n X_future
# K·ªãch b·∫£n A: D√πng Price_Index c≈©
X_future_A = df_future_2025[feature_cols[1:]].copy()
X_future_A['Price_Index'] = PRICE_INDEX_A
X_future_A = X_future_A[feature_cols]

# K·ªãch b·∫£n B: D√πng Price_Index t·ªëi ∆∞u
X_future_B = df_future_2025[feature_cols[1:]].copy()
X_future_B['Price_Index'] = PRICE_INDEX_B
X_future_B = X_future_B[feature_cols]

# L∆∞u tr·ªØ k·∫øt qu·∫£
forecast_results = pd.DataFrame(index=future_dates)
models_summary = {}

# ---------------------------------
# M√î H√åNH 1: LINEAR REGRESSION
# ---------------------------------
print("\n... [M√¥ h√¨nh 1] ƒêang ch·∫°y Linear Regression ...")
model_lr = LinearRegression()
model_lr.fit(X_train, Y_train)

pred_A_lr = model_lr.predict(X_future_A)
pred_B_lr = model_lr.predict(X_future_B)

forecast_results['LR_A'] = pred_A_lr
forecast_results['LR_B'] = pred_B_lr
models_summary['Linear Regression'] = (pred_A_lr.sum(), pred_B_lr.sum())

# ---------------------------------
# M√î H√åNH 2: RANDOM FOREST REGRESSOR
# ---------------------------------
print("... [M√¥ h√¨nh 2] ƒêang ch·∫°y Random Forest ...")
model_rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
model_rf.fit(X_train, Y_train)

pred_A_rf = model_rf.predict(X_future_A)
pred_B_rf = model_rf.predict(X_future_B)

forecast_results['RF_A'] = pred_A_rf
forecast_results['RF_B'] = pred_B_rf
models_summary['Random Forest'] = (pred_A_rf.sum(), pred_B_rf.sum())

# ---------------------------------
# M√î H√åNH 3: XGBOOST
# ---------------------------------
print("... [M√¥ h√¨nh 3] ƒêang ch·∫°y XGBoost ...")
model_xgb = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1, objective='reg:squarederror')
model_xgb.fit(X_train, Y_train)

pred_A_xgb = model_xgb.predict(X_future_A)
pred_B_xgb = model_xgb.predict(X_future_B)

forecast_results['XGB_A'] = pred_A_xgb
forecast_results['XGB_B'] = pred_B_xgb
models_summary['XGBoost'] = (pred_A_xgb.sum(), pred_B_xgb.sum())

# ---------------------------------
# M√î H√åNH 4: LSTM
# ---------------------------------
print("... [M√¥ h√¨nh 4] ƒêang ch·∫°y LSTM ...")
# 4.1. Chu·∫©n h√≥a d·ªØ li·ªáu cho LSTM
scaler_X = MinMaxScaler()
scaler_Y = MinMaxScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
Y_train_scaled = scaler_Y.fit_transform(Y_train.values.reshape(-1, 1))

# 4.2. ƒê·ªãnh h√¨nh l·∫°i d·ªØ li·ªáu
X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))

# 4.3. X√¢y d·ª±ng m√¥ h√¨nh
model_lstm = Sequential()
model_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mse')

# 4.4. Hu·∫•n luy·ªán
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=0)
model_lstm.fit(X_train_lstm, Y_train_scaled,
               epochs=20,
               batch_size=32,
               verbose=0,
               callbacks=[early_stop])

# 4.5. Chu·∫©n b·ªã d·ªØ li·ªáu t∆∞∆°ng lai
X_future_A_scaled = scaler_X.transform(X_future_A)
X_future_B_scaled = scaler_X.transform(X_future_B)

X_future_A_lstm = X_future_A_scaled.reshape((X_future_A_scaled.shape[0], 1, X_future_A_scaled.shape[1]))
X_future_B_lstm = X_future_B_scaled.reshape((X_future_B_scaled.shape[0], 1, X_future_B_scaled.shape[1]))

# 4.6. D·ª± b√°o
pred_A_lstm_scaled = model_lstm.predict(X_future_A_lstm, verbose=0)
pred_B_lstm_scaled = model_lstm.predict(X_future_B_lstm, verbose=0)

# 4.7. Chuy·ªÉn ƒë·ªïi ng∆∞·ª£c
pred_A_lstm = scaler_Y.inverse_transform(pred_A_lstm_scaled).flatten()
pred_B_lstm = scaler_Y.inverse_transform(pred_B_lstm_scaled).flatten()

forecast_results['LSTM_A'] = pred_A_lstm
forecast_results['LSTM_B'] = pred_B_lstm
models_summary['LSTM'] = (pred_A_lstm.sum(), pred_B_lstm.sum())

print("\n--- [HO√ÄN T·∫§T] ƒê√£ ch·∫°y xong 4 m√¥ h√¨nh! ---")


# --- B∆Ø·ªöC 4: T·ªîNG H·ª¢P K·∫æT QU·∫¢ V√Ä T√çNH L·ª¢I NHU·∫¨N ---
print("\n-- [B∆Ø·ªöC 4] ƒêang t·ªïng h·ª£p k·∫øt qu·∫£ v√† t√≠nh to√°n L·ª£i nhu·∫≠n... --")

df_summary_report = []

for model_name, (total_q_a, total_q_b) in models_summary.items():
    if total_q_a == 0 and total_q_b == 0:
        continue

    # --- K·ªãch b·∫£n A (Gi·ªØ gi√°) ---
    revenue_A = total_q_a * PRICE_PAID_A
    profit_A = total_q_a * (PRICE_PAID_A - NEW_COGS_PER_ITEM)

    # --- K·ªãch b·∫£n B (TƒÉng gi√°) ---
    revenue_B = total_q_b * PRICE_PAID_B
    profit_B = total_q_b * (PRICE_PAID_B - NEW_COGS_PER_ITEM)

    # --- Thay ƒë·ªïi ---
    profit_change = profit_B - profit_A

    df_summary_report.append({
        "Model": model_name,
        "Total_Profit_A (Gi·ªØ gi√°)": profit_A,
        "Total_Profit_B (TƒÉng gi√°)": profit_B,
        "L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m": profit_change,
        "Total_Quantity_A": total_q_a,
        "Total_Quantity_B": total_q_b
    })

df_final_report = pd.DataFrame(df_summary_report).set_index("Model")
df_final_report = df_final_report.sort_values(by="L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m", ascending=False)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
display(Markdown("\n" + "="*50))
display(Markdown("üìä B√ÅO C√ÅO T·ªîNG H·ª¢P D·ª∞ B√ÅO L·ª¢I NHU·∫¨N Q1/2025 (4 M√î H√åNH)"))
display(Markdown(f"D·ª± b√°o cho {N_DAYS_FORECAST} ng√†y (Q1/2025) khi COGS m·ªõi l√† {NEW_COGS_PER_ITEM:,.0f} ƒë/ly."))
display(Markdown("="*50 + "\n"))

# Format cho ƒë·∫πp
format_dict = {
    "Total_Profit_A (Gi·ªØ gi√°)": "{:,.0f} ƒë",
    "Total_Profit_B (TƒÉng gi√°)": "{:,.0f} ƒë",
    "L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m": "{:,.0f} ƒë",
    "Total_Quantity_A": "{:,.0f} ly",
    "Total_Quantity_B": "{:,.0f} ly"
}
# [S·ª¨A L·ªñI] D√πng display() v·ªõi .style, kh√¥ng d√πng print()
display(df_final_report.style.format(format_dict).background_gradient(cmap='RdYlGn', subset=['L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m']))


# --- B∆Ø·ªöC 5: TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ ---
print("\n-- [B∆Ø·ªöC 5] ƒêang v·∫Ω bi·ªÉu ƒë·ªì k·∫øt qu·∫£... --")

# L·∫•y k·∫øt qu·∫£ t·ª´ m√¥ h√¨nh t·ªët nh·∫•t (v√≠ d·ª•: XGBoost)
best_model_name = 'XGBoost'
best_model_prefix = 'XGB'

plt.figure(figsize=(15, 7))
forecast_results[f'{best_model_prefix}_A'].plot(label=f'K·ªãch b·∫£n A: Gi·ªØ gi√° (L∆∞·ª£ng b√°n)', style='b--')
forecast_results[f'{best_model_prefix}_B'].plot(label=f'K·ªãch b·∫£n B: TƒÉng gi√° (L∆∞·ª£ng b√°n)', style='g-')
plt.title(f'D·ª± b√°o L∆∞·ª£ng b√°n Q1/2025 (S·ª≠ d·ª•ng m√¥ h√¨nh: {best_model_name})')
plt.ylabel('T·ªïng l∆∞·ª£ng b√°n h√†ng ng√†y')
plt.xlabel('Ng√†y')
plt.legend()
plt.grid(True)
plt.savefig('forecast_comparison_chart_4_models.png')
print("Ho√†n t·∫•t. ƒê√£ l∆∞u bi·ªÉu ƒë·ªì v√†o 'forecast_comparison_chart_4_models.png'.")

# --- [FILE STANDALONE]: ƒê√ÅNH GI√Å MODEL (R2, MAPE) & D·ª∞ B√ÅO K·ªäCH B·∫¢N ---
# Giai ƒëo·∫°n A: ƒê√°nh gi√° model tr√™n d·ªØ li·ªáu 2024
# Giai ƒëo·∫°n B: D·ª± b√°o 2 k·ªãch b·∫£n cho Q1/2025
# -----------------------------------------------------------------

# --- B∆Ø·ªöC 0: IMPORT TH∆Ø VI·ªÜN ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import timedelta
import warnings
from IPython.display import display, Markdown

# Th∆∞ vi·ªán Preprocessing
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Th∆∞ vi·ªán 4 M√¥ h√¨nh
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
# T·∫Øt c·∫£nh b√°o TensorFlow
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
tf.get_logger().setLevel('ERROR')
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# [M·ªöI] Th∆∞ vi·ªán ƒê√°nh gi√°
from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

# C√†i ƒë·∫∑t
pd.set_option('display.float_format', '{:.2f}'.format)
warnings.filterwarnings('ignore')
print("-- [B∆Ø·ªöC 0] T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c nh·∫≠p th√†nh c√¥ng. --")


# --- [GI·∫¢ ƒê·ªäNH K·ªäCH B·∫¢N] ---
# (C√°c gi·∫£ ƒë·ªãnh n√†y d√πng cho Giai ƒëo·∫°n B)
PRICE_INDEX_A = 0.95
PRICE_INDEX_B = 1.02
AVG_LIST_PRICE_PER_ITEM = 55000
NEW_COGS_PER_ITEM = 18000

PRICE_PAID_A = AVG_LIST_PRICE_PER_ITEM * PRICE_INDEX_A
PRICE_PAID_B = AVG_LIST_PRICE_PER_ITEM * PRICE_INDEX_B
print(f"-- [GI·∫¢ ƒê·ªäNH] K·ªãch b·∫£n A (Gi·ªØ gi√°): {PRICE_PAID_A:,.0f} ƒë / 1 ly")
print(f"-- [GI·∫¢ ƒê·ªäNH] K·ªãch b·∫£n B (TƒÉng gi√°): {PRICE_PAID_B:,.0f} ƒë / 1 ly")
print(f"-- [GI·∫¢ ƒê·ªäNH] Gi√° v·ªën m·ªõi: {NEW_COGS_PER_ITEM:,.0f} ƒë / 1 ly")
# ----------------------------------


# --- B∆Ø·ªöC 1: T·∫¢I V√Ä T·ªîNG H·ª¢P D·ªÆ LI·ªÜU (T·∫†O B·∫¢NG HU·∫§N LUY·ªÜN 2024) ---
print("\n-- [B∆Ø·ªöC 1] ƒêang t·∫£i v√† t·∫°o B·∫£ng d·ªØ li·ªáu hu·∫•n luy·ªán (2024)... --")
try:
    df_trans = pd.read_csv('transaction_data.csv')
    df_prod = pd.read_csv('product_master.csv') # C·∫ßn ƒë·ªÉ l·∫•y gi√° ni√™m y·∫øt
    df_macro = pd.read_csv('macro_context.csv')
except FileNotFoundError:
    print("L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp CSV. Vui l√≤ng ƒë·∫£m b·∫£o 3 t·ªáp (trans, prod, macro) n·∫±m trong c√πng th∆∞ m·ª•c.")
    # exit()

# (Th·ª±c hi·ªán c√°c b∆∞·ªõc chu·∫©n b·ªã d·ªØ li·ªáu nh∆∞ tr∆∞·ªõc)
df_trans['Date_Time'] = pd.to_datetime(df_trans['Date_Time'])
df_trans['Date'] = df_trans['Date_Time'].dt.date.astype(str)
df_macro['Date'] = pd.to_datetime(df_macro['Date']).dt.date.astype(str)

df_trans = pd.merge(
    df_trans,
    df_prod[['Product_ID', 'Unit_Price_List']],
    on='Product_ID',
    how='left'
)
df_trans = df_trans.rename(columns={'Unit_Price_Listed': 'Unit_Price_Recorded', 'Unit_Price_List': 'Unit_Price_Master'})
df_trans['Unit_Price_Master'] = df_trans['Unit_Price_Master'].fillna(df_trans['Unit_Price_Recorded'])
df_trans['Total_List_Price'] = df_trans['Quantity'] * df_trans['Unit_Price_Master']

df_daily_agg = df_trans.groupby('Date').agg(
    Total_Quantity=('Quantity', 'sum'),
    Total_Paid_Agg=('Total_Paid', 'sum'),
    Total_List_Price_Agg=('Total_List_Price', 'sum')
).reset_index()

df_daily_agg = df_daily_agg[df_daily_agg['Total_List_Price_Agg'] > 0]
df_daily_agg['Price_Index'] = df_daily_agg['Total_Paid_Agg'] / df_daily_agg['Total_List_Price_Agg']

df_train_2024_full = pd.merge(
    df_daily_agg,
    df_macro[['Date', 'Is_Weekend', 'Is_Holiday', 'Promotion_Campaign']],
    on='Date',
    how='left'
)

df_train_2024_full['Date'] = pd.to_datetime(df_train_2024_full['Date'])
df_train_2024_full = df_train_2024_full.set_index('Date')
df_train_2024_full['Month'] = df_train_2024_full.index.month
df_train_2024_full['DayOfWeek'] = df_train_2024_full.index.dayofweek

df_train_2024_full = df_train_2024_full.dropna()
print(f"Ho√†n t·∫•t B∆Ø·ªöC 1. ƒê√£ t·∫°o b·∫£ng d·ªØ li·ªáu 2024 (full) v·ªõi {len(df_train_2024_full)} ng√†y.")


# --- GIAI ƒêO·∫†N A: ƒê√ÅNH GI√Å M√î H√åNH (TR√äN D·ªÆ LI·ªÜU 2024) ---

print("\n\n" + "="*50)
print(" GIAI ƒêO·∫†N A: ƒê√ÅNH GI√Å M√î H√åNH (R2, MAPE) TR√äN D·ªÆ LI·ªÜU 2024")
print("="*50)

# --- B∆Ø·ªöC 2: CHIA D·ªÆ LI·ªÜU 2024 TH√ÄNH T·∫¨P TRAIN V√Ä TEST ---
print("\n-- [B∆Ø·ªöC 2] ƒêang chia d·ªØ li·ªáu 2024 th√†nh Train (10 th√°ng) v√† Test (2 th√°ng)... --")
split_date = '2024-11-01'
train_data = df_train_2024_full.loc[df_train_2024_full.index < split_date]
test_data = df_train_2024_full.loc[df_train_2024_full.index >= split_date]

target_col = 'Total_Quantity'
feature_cols = ['Price_Index', 'Is_Weekend', 'Is_Holiday', 'Promotion_Campaign', 'Month', 'DayOfWeek']

Y_train = train_data[target_col]
X_train = train_data[feature_cols]
Y_test = test_data[target_col]
X_test = test_data[feature_cols]

print(f" - K√≠ch th∆∞·ªõc t·∫≠p Train: {len(X_train)} ng√†y")
print(f" - K√≠ch th∆∞·ªõc t·∫≠p Test: {len(X_test)} ng√†y")

# --- B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN, D·ª∞ B√ÅO (TEST SET) V√Ä T√çNH TO√ÅN CH·ªà S·ªê ---
print("\n-- [B∆Ø·ªöC 3] ƒêang hu·∫•n luy·ªán v√† ƒë√°nh gi√° 4 m√¥ h√¨nh tr√™n t·∫≠p Test... --")

evaluation_results = []
test_predictions = pd.DataFrame(index=Y_test.index)
test_predictions['Actual'] = Y_test

def calculate_metrics(model_name, y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred) * 100
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    print(f"... Ho√†n t·∫•t: {model_name} (R2: {r2:.2f}, MAPE: {mape:.2f}%)")
    evaluation_results.append({
        "Model": model_name,
        "R-squared": r2,
        "MAPE (%)": mape,
        "MAE (ly)": mae,
        "RMSE (ly)": rmse
    })
    return y_pred

# --- 4 Models ---
# 1. Linear Regression
model_lr = LinearRegression()
model_lr.fit(X_train, Y_train)
pred_lr = model_lr.predict(X_test)
test_predictions['LR'] = calculate_metrics("Linear Regression", Y_test, pred_lr)

# 2. Random Forest
model_rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
model_rf.fit(X_train, Y_train)
pred_rf = model_rf.predict(X_test)
test_predictions['RF'] = calculate_metrics("Random Forest", Y_test, pred_rf)

# 3. XGBoost
model_xgb = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1, objective='reg:squarederror')
model_xgb.fit(X_train, Y_train)
pred_xgb = model_xgb.predict(X_test)
test_predictions['XGB'] = calculate_metrics("XGBoost", Y_test, pred_xgb)

# 4. LSTM
scaler_X_eval = MinMaxScaler()
scaler_Y_eval = MinMaxScaler()
X_train_scaled_eval = scaler_X_eval.fit_transform(X_train)
Y_train_scaled_eval = scaler_Y_eval.fit_transform(Y_train.values.reshape(-1, 1))
X_test_scaled_eval = scaler_X_eval.transform(X_test)

X_train_lstm_eval = X_train_scaled_eval.reshape((X_train_scaled_eval.shape[0], 1, X_train_scaled_eval.shape[1]))
X_test_lstm_eval = X_test_scaled_eval.reshape((X_test_scaled_eval.shape[0], 1, X_test_scaled_eval.shape[1]))

model_lstm = Sequential()
model_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_lstm_eval.shape[1], X_train_lstm_eval.shape[2])))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mse')
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=0)
model_lstm.fit(X_train_lstm_eval, Y_train_scaled_eval, epochs=20, batch_size=32, verbose=0, callbacks=[early_stop])

pred_lstm_scaled = model_lstm.predict(X_test_lstm_eval, verbose=0)
pred_lstm = scaler_Y_eval.inverse_transform(pred_lstm_scaled).flatten()
test_predictions['LSTM'] = calculate_metrics("LSTM", Y_test, pred_lstm)

# --- B∆Ø·ªöC 4: B√ÅO C√ÅO ƒê√ÅNH GI√Å (R2, MAPE) ---
df_evaluation_report = pd.DataFrame(evaluation_results).set_index("Model")
df_evaluation_report = df_evaluation_report.sort_values(by="MAPE (%)", ascending=True)

display(Markdown("\n" + "="*50))
display(Markdown("üìä B√ÅO C√ÅO ƒê√ÅNH GI√Å M√î H√åNH (tr√™n t·∫≠p Test Nov-Dec 2024)"))
display(Markdown("="*50 + "\n"))
display(df_evaluation_report.style.format({
    "R-squared": "{:.3f}",
    "MAPE (%)": "{:.2f}%",
    "MAE (ly)": "{:,.1f}",
    "RMSE (ly)": "{:,.1f}"
}).background_gradient(cmap='RdYlGn_r', subset=['MAPE (%)']))


# --- B∆Ø·ªöC 5: TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ ƒê√ÅNH GI√Å (CHI TI·∫æT 4 MODEL) ---
print("\n-- [B∆Ø·ªöC 5] ƒêang tr·ª±c quan h√≥a chi ti·∫øt 4 model (Actual vs Predicted)... --")

fig, axes = plt.subplots(4, 1, figsize=(15, 20), sharex=True)
fig.suptitle('ƒê√ÅNH GI√Å CHI TI·∫æT 4 MODEL (T·∫≠p Test Nov-Dec 2024)', fontsize=16, y=1.02)

model_names = [
    ("Linear Regression", "LR"),
    ("Random Forest", "RF"),
    ("XGBoost", "XGB"),
    ("LSTM", "LSTM")
]

for i, (full_name, short_name) in enumerate(model_names):
    ax = axes[i]
    test_predictions['Actual'].plot(ax=ax, label='S·∫£n l∆∞·ª£ng Th·ª±c t·∫ø (Actual)', style='k-', linewidth=2)
    test_predictions[short_name].plot(ax=ax, label=f'D·ª± b√°o ({full_name})', style='r--', linewidth=2)

    # L·∫•y MAPE t·ª´ b·∫£ng b√°o c√°o
    mape_score = df_evaluation_report.loc[full_name]["MAPE (%)"]

    ax.set_title(f'{i+1}. {full_name} (MAPE: {mape_score:.2f}%)')
    ax.set_ylabel('T·ªïng l∆∞·ª£ng b√°n h√†ng ng√†y')
    ax.legend()
    ax.grid(True)

plt.xlabel('Ng√†y')
plt.tight_layout()
plt.savefig('evaluation_chart_grid_4_models.png')
print("Ho√†n t·∫•t. ƒê√£ l∆∞u bi·ªÉu ƒë·ªì l∆∞·ªõi v√†o 'evaluation_chart_grid_4_models.png'.")
plt.show()


# --- GIAI ƒêO·∫†N B: D·ª∞ B√ÅO K·ªäCH B·∫¢N (CHO Q1/2025) ---

print("\n\n" + "="*50)
print(" GIAI ƒêO·∫†N B: D·ª∞ B√ÅO K·ªäCH B·∫¢N (CHO Q1/2025)")
print("="*50)

# --- B∆Ø·ªöC 6: HU·∫§N LUY·ªÜN L·∫†I TR√äN TO√ÄN B·ªò D·ªÆ LI·ªÜU 2024 V√Ä D·ª∞ B√ÅO ---
print("\n-- [B∆Ø·ªöC 6] ƒêang hu·∫•n luy·ªán l·∫°i 4 m√¥ h√¨nh tr√™n TO√ÄN B·ªò d·ªØ li·ªáu 2024... --")

# D·ªØ li·ªáu hu·∫•n luy·ªán (full)
Y_train_full = df_train_2024_full[target_col]
X_train_full = df_train_2024_full[feature_cols]

# D·ªØ li·ªáu t∆∞∆°ng lai (Q1/2025)
df_future_2025 = pd.DataFrame(index=pd.date_range(start='2025-01-01', periods=90, freq='D'))
df_future_2025['Is_Weekend'] = (df_future_2025.index.dayofweek >= 5).astype(int)
df_future_2025['Month'] = df_future_2025.index.month
df_future_2025['DayOfWeek'] = df_future_2025.index.dayofweek
df_future_2025['Is_Holiday'] = 0
df_future_2025['Promotion_Campaign'] = 0

# T·∫°o 2 k·ªãch b·∫£n X_future
X_future_A = df_future_2025.copy()
X_future_A['Price_Index'] = PRICE_INDEX_A
X_future_A = X_future_A[feature_cols]

X_future_B = df_future_2025.copy()
X_future_B['Price_Index'] = PRICE_INDEX_B
X_future_B = X_future_B[feature_cols]

# L∆∞u tr·ªØ k·∫øt qu·∫£
forecast_results = pd.DataFrame(index=df_future_2025.index)
models_summary = {}

# --- 4 Models (Re-training) ---
print("... [1/4] Re-train Linear Regression ...")
model_lr.fit(X_train_full, Y_train_full)
pred_A_lr = model_lr.predict(X_future_A)
pred_B_lr = model_lr.predict(X_future_B)
forecast_results['LR_A'] = pred_A_lr
forecast_results['LR_B'] = pred_B_lr
models_summary['Linear Regression'] = (pred_A_lr.sum(), pred_B_lr.sum())

print("... [2/4] Re-train Random Forest ...")
model_rf.fit(X_train_full, Y_train_full)
pred_A_rf = model_rf.predict(X_future_A)
pred_B_rf = model_rf.predict(X_future_B)
forecast_results['RF_A'] = pred_A_rf
forecast_results['RF_B'] = pred_B_rf
models_summary['Random Forest'] = (pred_A_rf.sum(), pred_B_rf.sum())

print("... [3/4] Re-train XGBoost ...")
model_xgb.fit(X_train_full, Y_train_full)
pred_A_xgb = model_xgb.predict(X_future_A)
pred_B_xgb = model_xgb.predict(X_future_B)
forecast_results['XGB_A'] = pred_A_xgb
forecast_results['XGB_B'] = pred_B_xgb
models_summary['XGBoost'] = (pred_A_xgb.sum(), pred_B_xgb.sum())

print("... [4/4] Re-train LSTM ...")
scaler_X_full = MinMaxScaler()
scaler_Y_full = MinMaxScaler()
X_train_scaled_full = scaler_X_full.fit_transform(X_train_full)
Y_train_scaled_full = scaler_Y_full.fit_transform(Y_train_full.values.reshape(-1, 1))

X_train_lstm_full = X_train_scaled_full.reshape((X_train_scaled_full.shape[0], 1, X_train_scaled_full.shape[1]))

model_lstm.fit(X_train_lstm_full, Y_train_scaled_full, epochs=20, batch_size=32, verbose=0, callbacks=[early_stop])

X_future_A_scaled = scaler_X_full.transform(X_future_A)
X_future_B_scaled = scaler_X_full.transform(X_future_B)
X_future_A_lstm = X_future_A_scaled.reshape((X_future_A_scaled.shape[0], 1, X_future_A_scaled.shape[1]))
X_future_B_lstm = X_future_B_scaled.reshape((X_future_B_scaled.shape[0], 1, X_future_B_scaled.shape[1]))

pred_A_lstm_scaled = model_lstm.predict(X_future_A_lstm, verbose=0)
pred_B_lstm_scaled = model_lstm.predict(X_future_B_lstm, verbose=0)

pred_A_lstm = scaler_Y_full.inverse_transform(pred_A_lstm_scaled).flatten()
pred_B_lstm = scaler_Y_full.inverse_transform(pred_B_lstm_scaled).flatten()
forecast_results['LSTM_A'] = pred_A_lstm
forecast_results['LSTM_B'] = pred_B_lstm
models_summary['LSTM'] = (pred_A_lstm.sum(), pred_B_lstm.sum())

print("\n--- [HO√ÄN T·∫§T] ƒê√£ ch·∫°y xong 4 m√¥ h√¨nh d·ª± b√°o! ---")


# --- B∆Ø·ªöC 7: T·ªîNG H·ª¢P L·ª¢I NHU·∫¨N (Q1/2025) ---
print("\n-- [B∆Ø·ªöC 7] ƒêang t·ªïng h·ª£p k·∫øt qu·∫£ v√† t√≠nh to√°n L·ª£i nhu·∫≠n Q1/2025... --")

df_summary_report = []
for model_name, (total_q_a, total_q_b) in models_summary.items():
    revenue_A = total_q_a * PRICE_PAID_A
    profit_A = total_q_a * (PRICE_PAID_A - NEW_COGS_PER_ITEM)

    revenue_B = total_q_b * PRICE_PAID_B
    profit_B = total_q_b * (PRICE_PAID_B - NEW_COGS_PER_ITEM)

    profit_change = profit_B - profit_A

    df_summary_report.append({
        "Model": model_name,
        "Total_Profit_A (Gi·ªØ gi√°)": profit_A,
        "Total_Profit_B (TƒÉng gi√°)": profit_B,
        "L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m": profit_change,
        "Total_Quantity_A": total_q_a,
        "Total_Quantity_B": total_q_b
    })

df_final_report = pd.DataFrame(df_summary_report).set_index("Model")
df_final_report = df_final_report.sort_values(by="L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m", ascending=False)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
display(Markdown("\n" + "="*50))
display(Markdown("üìä B√ÅO C√ÅO T·ªîNG H·ª¢P D·ª∞ B√ÅO L·ª¢I NHU·∫¨N Q1/2025 (4 M√î H√åNH)"))
display(Markdown(f"D·ª± b√°o cho {N_DAYS_FORECAST} ng√†y (Q1/2025) khi COGS m·ªõi l√† {NEW_COGS_PER_ITEM:,.0f} ƒë/ly."))
display(Markdown("="*50 + "\n"))

format_dict = {
    "Total_Profit_A (Gi·ªØ gi√°)": "{:,.0f} ƒë",
    "Total_Profit_B (TƒÉng gi√°)": "{:,.0f} ƒë",
    "L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m": "{:,.0f} ƒë",
    "Total_Quantity_A": "{:,.0f} ly",
    "Total_Quantity_B": "{:,.0f} ly"
}
display(df_final_report.style.format(format_dict).background_gradient(cmap='RdYlGn', subset=['L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m']))


# --- B∆Ø·ªöC 8: TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ D·ª∞ B√ÅO (Q1/2025) ---
print("\n-- [B∆Ø·ªöC 8] ƒêang v·∫Ω bi·ªÉu ƒë·ªì d·ª± b√°o (Q1/2025)... --")

best_model_name_forecast = df_final_report.index[0]
prefix_map = {'Linear': 'LR', 'Random': 'RF', 'XGBoost': 'XGB', 'LSTM': 'LSTM'}
best_model_prefix = prefix_map.get(best_model_name_forecast.split(" ")[0], 'XGB')

plt.figure(figsize=(15, 7))
forecast_results[f'{best_model_prefix}_A'].plot(label=f'K·ªãch b·∫£n A: Gi·ªØ gi√° (L∆∞·ª£ng b√°n)', style='b--')
forecast_results[f'{best_model_prefix}_B'].plot(label=f'K·ªãch b·∫£n B: TƒÉng gi√° (L∆∞·ª£ng b√°n)', style='g-')
plt.title(f'D·ª± b√°o L∆∞·ª£ng b√°n Q1/2025 (S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t: {best_model_name_forecast})')
plt.ylabel('T·ªïng l∆∞·ª£ng b√°n h√†ng ng√†y')
plt.xlabel('Ng√†y')
plt.legend()
plt.grid(True)
plt.savefig('forecast_comparison_chart_4_models.png')
print("Ho√†n t·∫•t. ƒê√£ l∆∞u bi·ªÉu ƒë·ªì v√†o 'forecast_comparison_chart_4_models.png'.")
plt.show()

# --- [FILE STANDALONE]: TINH CH·ªàNH S√ÇU (ADVANCED TUNING) & D·ª∞ B√ÅO ---
# Giai ƒëo·∫°n A: ƒê√°nh gi√° model (R2, MAPE) V·ªöI TimeSeriesSplit V√Ä TUNING S√ÇU
# Giai ƒëo·∫°n B: D·ª± b√°o 2 k·ªãch b·∫£n cho Q1/2025 V·ªöI MODEL ƒê√É TUNING
# -----------------------------------------------------------------

# --- B∆Ø·ªöC 0: IMPORT TH∆Ø VI·ªÜN ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import timedelta
import warnings
from IPython.display import display, Markdown

# Th∆∞ vi·ªán Preprocessing
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# [M·ªöI] Th∆∞ vi·ªán Tinh ch·ªânh
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import TimeSeriesSplit # [QUAN TR·ªåNG] D√πng cho chu·ªói th·ªùi gian

# Th∆∞ vi·ªán 4 M√¥ h√¨nh
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
# T·∫Øt c·∫£nh b√°o TensorFlow
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
tf.get_logger().setLevel('ERROR')
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# Th∆∞ vi·ªán ƒê√°nh gi√°
from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

# C√†i ƒë·∫∑t
pd.set_option('display.float_format', '{:.2f}'.format)
warnings.filterwarnings('ignore')
print("-- [B∆Ø·ªöC 0] T·∫•t c·∫£ th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c nh·∫≠p th√†nh c√¥ng. --")


# --- [GI·∫¢ ƒê·ªäNH K·ªäCH B·∫¢N] ---
# (C√°c gi·∫£ ƒë·ªãnh n√†y d√πng cho Giai ƒëo·∫°n B)
PRICE_INDEX_A = 0.95
PRICE_INDEX_B = 1.02
AVG_LIST_PRICE_PER_ITEM = 55000
NEW_COGS_PER_ITEM = 18000

PRICE_PAID_A = AVG_LIST_PRICE_PER_ITEM * PRICE_INDEX_A
PRICE_PAID_B = AVG_LIST_PRICE_PER_ITEM * PRICE_INDEX_B
print(f"-- [GI·∫¢ ƒê·ªäNH] K·ªãch b·∫£n A (Gi·ªØ gi√°): {PRICE_PAID_A:,.0f} ƒë / 1 ly")
print(f"-- [GI·∫¢ ƒê·ªäNH] K·ªãch b·∫£n B (TƒÉng gi√°): {PRICE_PAID_B:,.0f} ƒë / 1 ly")
print(f"-- [GI·∫¢ ƒê·ªäNH] Gi√° v·ªën m·ªõi: {NEW_COGS_PER_ITEM:,.0f} ƒë / 1 ly")
# ----------------------------------


# --- B∆Ø·ªöC 1: T·∫¢I V√Ä T·ªîNG H·ª¢P D·ªÆ LI·ªÜU (T·∫†O B·∫¢NG HU·∫§N LUY·ªÜN 2024) ---
print("\n-- [B∆Ø·ªöC 1] ƒêang t·∫£i v√† t·∫°o B·∫£ng d·ªØ li·ªáu hu·∫•n luy·ªán (2024)... --")
try:
    df_trans = pd.read_csv('transaction_data.csv')
    df_prod = pd.read_csv('product_master.csv') # C·∫ßn ƒë·ªÉ l·∫•y gi√° ni√™m y·∫øt
    df_macro = pd.read_csv('macro_context.csv')
except FileNotFoundError:
    print("L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp CSV. Vui l√≤ng ƒë·∫£m b·∫£o 3 t·ªáp (trans, prod, macro) n·∫±m trong c√πng th∆∞ m·ª•c.")
    # exit()

# (Th·ª±c hi·ªán c√°c b∆∞·ªõc chu·∫©n b·ªã d·ªØ li·ªáu nh∆∞ tr∆∞·ªõc)
df_trans['Date_Time'] = pd.to_datetime(df_trans['Date_Time'])
df_trans['Date'] = df_trans['Date_Time'].dt.date.astype(str)
df_macro['Date'] = pd.to_datetime(df_macro['Date']).dt.date.astype(str)

df_trans = pd.merge(
    df_trans,
    df_prod[['Product_ID', 'Unit_Price_List']],
    on='Product_ID',
    how='left'
)
df_trans = df_trans.rename(columns={'Unit_Price_Listed': 'Unit_Price_Recorded', 'Unit_Price_List': 'Unit_Price_Master'})
df_trans['Unit_Price_Master'] = df_trans['Unit_Price_Master'].fillna(df_trans['Unit_Price_Recorded'])
df_trans['Total_List_Price'] = df_trans['Quantity'] * df_trans['Unit_Price_Master']

df_daily_agg = df_trans.groupby('Date').agg(
    Total_Quantity=('Quantity', 'sum'),
    Total_Paid_Agg=('Total_Paid', 'sum'),
    Total_List_Price_Agg=('Total_List_Price', 'sum')
).reset_index()

df_daily_agg = df_daily_agg[df_daily_agg['Total_List_Price_Agg'] > 0]
df_daily_agg['Price_Index'] = df_daily_agg['Total_Paid_Agg'] / df_daily_agg['Total_List_Price_Agg']

df_train_2024_full = pd.merge(
    df_daily_agg,
    df_macro[['Date', 'Is_Weekend', 'Is_Holiday', 'Promotion_Campaign']],
    on='Date',
    how='left'
)

df_train_2024_full['Date'] = pd.to_datetime(df_train_2024_full['Date'])
df_train_2024_full = df_train_2024_full.set_index('Date')
df_train_2024_full['Month'] = df_train_2024_full.index.month
df_train_2024_full['DayOfWeek'] = df_train_2024_full.index.dayofweek

df_train_2024_full = df_train_2024_full.dropna()
print(f"Ho√†n t·∫•t B∆Ø·ªöC 1. ƒê√£ t·∫°o b·∫£ng d·ªØ li·ªáu 2024 (full) v·ªõi {len(df_train_2024_full)} ng√†y.")


# --- GIAI ƒêO·∫†N A: ƒê√ÅNH GI√Å M√î H√åNH (V·ªöI TUNING S√ÇU) ---

print("\n\n" + "="*50)
print(" GIAI ƒêO·∫†N A: ƒê√ÅNH GI√Å M√î H√åNH (R2, MAPE) V·ªöI TUNING S√ÇU")
print("="*50)

# --- B∆Ø·ªöC 2: CHIA D·ªÆ LI·ªÜU 2024 TH√ÄNH T·∫¨P TRAIN V√Ä TEST ---
print("\n-- [B∆Ø·ªöC 2] ƒêang chia d·ªØ li·ªáu 2024 th√†nh Train (10 th√°ng) v√† Test (2 th√°ng)... --")
split_date = '2024-11-01'
train_data = df_train_2024_full.loc[df_train_2024_full.index < split_date]
test_data = df_train_2024_full.loc[df_train_2024_full.index >= split_date]

target_col = 'Total_Quantity'
feature_cols = ['Price_Index', 'Is_Weekend', 'Is_Holiday', 'Promotion_Campaign', 'Month', 'DayOfWeek']

Y_train = train_data[target_col]
X_train = train_data[feature_cols]
Y_test = test_data[target_col]
X_test = test_data[feature_cols]

print(f" - K√≠ch th∆∞·ªõc t·∫≠p Train: {len(X_train)} ng√†y")
print(f" - K√≠ch th∆∞·ªõc t·∫≠p Test: {len(X_test)} ng√†y")

# --- B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN, D·ª∞ B√ÅO (TEST SET) V√Ä T√çNH TO√ÅN CH·ªà S·ªê ---
print("\n-- [B∆Ø·ªöC 3] ƒêang hu·∫•n luy·ªán, tinh ch·ªânh v√† ƒë√°nh gi√° 4 m√¥ h√¨nh tr√™n t·∫≠p Test... --")

evaluation_results = []
test_predictions = pd.DataFrame(index=Y_test.index)
test_predictions['Actual'] = Y_test

# Bi·∫øn ƒë·ªÉ l∆∞u c√°c m√¥ h√¨nh/tham s·ªë t·ªët nh·∫•t cho Giai ƒëo·∫°n B
best_model_objects = {}

# [M·ªöI] ƒê·ªãnh nghƒ©a TimeSeriesSplit ƒë·ªÉ ki·ªÉm tra ch√©o (CV)
# S·∫Ω chia t·∫≠p X_train (10 th√°ng) th√†nh 3 ph·∫ßn g·ªëi ƒë·∫ßu nhau
tscv = TimeSeriesSplit(n_splits=3)

def calculate_metrics(model_name, y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred) * 100
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    print(f"... Ho√†n t·∫•t: {model_name} (R2: {r2:.2f}, MAPE: {mape:.2f}%)")
    evaluation_results.append({
        "Model": model_name,
        "R-squared": r2,
        "MAPE (%)": mape,
        "MAE (ly)": mae,
        "RMSE (ly)": rmse
    })
    return y_pred

# --- 4 Models ---
# 1. Linear Regression
print("\n... [1/4] ƒêang ch·∫°y Linear Regression (Baseline) ...")
model_lr = LinearRegression()
model_lr.fit(X_train, Y_train)
pred_lr = model_lr.predict(X_test)
test_predictions['LR'] = calculate_metrics("Linear Regression", Y_test, pred_lr)
best_model_objects['LR'] = model_lr # L∆∞u l·∫°i model

# 2. Random Forest (V·ªöI TUNING S√ÇU H∆†N)
print("\n... [2/4] ƒêang *TINH CH·ªàNH S√ÇU* (Tuning) Random Forest ...")
param_dist_rf = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_leaf': [1, 2, 4, 6],
    'min_samples_split': [2, 5, 10],
    'max_features': ['sqrt', 'log2', 1.0] # 1.0 l√† t∆∞∆°ng ƒë∆∞∆°ng "auto" c≈©
}
model_rf = RandomForestRegressor(random_state=42)
random_search_rf = RandomizedSearchCV(
    estimator=model_rf,
    param_distributions=param_dist_rf,
    n_iter=30, # [M·ªöI] TƒÉng s·ªë l·∫ßn th·ª≠ l√™n 30
    cv=tscv,   # [M·ªöI] D√πng TimeSeriesSplit
    scoring='neg_mean_absolute_percentage_error', # T·ªëi ∆∞u theo MAPE
    random_state=42,
    n_jobs=-1,
    verbose=0
)
random_search_rf.fit(X_train, Y_train)
best_model_rf = random_search_rf.best_estimator_ # ƒê√¢y l√† model RF t·ªët nh·∫•t
pred_rf = best_model_rf.predict(X_test)
test_predictions['RF'] = calculate_metrics("Random Forest (Tuned)", Y_test, pred_rf)
best_model_objects['RF'] = best_model_rf # L∆∞u l·∫°i model T·ªêT NH·∫§T

# 3. XGBoost (V·ªöI TUNING S√ÇU H∆†N)
print("\n... [3/4] ƒêang *TINH CH·ªàNH S√ÇU* (Tuning) XGBoost ...")
param_dist_xgb = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 5, 7, 9, 11],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2]
}
model_xgb = XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror')
random_search_xgb = RandomizedSearchCV(
    estimator=model_xgb,
    param_distributions=param_dist_xgb,
    n_iter=30, # [M·ªöI] TƒÉng s·ªë l·∫ßn th·ª≠ l√™n 30
    cv=tscv,   # [M·ªöI] D√πng TimeSeriesSplit
    scoring='neg_mean_absolute_percentage_error',
    random_state=42,
    n_jobs=-1,
    verbose=0
)
random_search_xgb.fit(X_train, Y_train)
best_model_xgb = random_search_xgb.best_estimator_ # ƒê√¢y l√† model XGB t·ªët nh·∫•t
pred_xgb = best_model_xgb.predict(X_test)
test_predictions['XGB'] = calculate_metrics("XGBoost (Tuned)", Y_test, pred_xgb)
best_model_objects['XGB'] = best_model_xgb # L∆∞u l·∫°i model T·ªêT NH·∫§T

# 4. LSTM
print("\n... [4/4] ƒêang ch·∫°y LSTM (Baseline) ...")
scaler_X_eval = MinMaxScaler()
scaler_Y_eval = MinMaxScaler()
X_train_scaled_eval = scaler_X_eval.fit_transform(X_train)
Y_train_scaled_eval = scaler_Y_eval.fit_transform(Y_train.values.reshape(-1, 1))
X_test_scaled_eval = scaler_X_eval.transform(X_test)
X_train_lstm_eval = X_train_scaled_eval.reshape((X_train_scaled_eval.shape[0], 1, X_train_scaled_eval.shape[1]))
X_test_lstm_eval = X_test_scaled_eval.reshape((X_test_scaled_eval.shape[0], 1, X_test_scaled_eval.shape[1]))
model_lstm = Sequential()
model_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_lstm_eval.shape[1], X_train_lstm_eval.shape[2])))
model_lstm.add(Dense(1))
model_lstm.compile(optimizer='adam', loss='mse')
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=0)
model_lstm.fit(X_train_lstm_eval, Y_train_scaled_eval, epochs=30, batch_size=32, verbose=0, callbacks=[early_stop]) # TƒÉng epochs
pred_lstm_scaled = model_lstm.predict(X_test_lstm_eval, verbose=0)
pred_lstm = scaler_Y_eval.inverse_transform(pred_lstm_scaled).flatten()
test_predictions['LSTM'] = calculate_metrics("LSTM", Y_test, pred_lstm)

# --- B∆Ø·ªöC 4: B√ÅO C√ÅO ƒê√ÅNH GI√Å (R2, MAPE) ---
df_evaluation_report = pd.DataFrame(evaluation_results).set_index("Model")
df_evaluation_report = df_evaluation_report.sort_values(by="MAPE (%)", ascending=True)

display(Markdown("\n" + "="*50))
display(Markdown("üìä B√ÅO C√ÅO ƒê√ÅNH GI√Å M√î H√åNH (tr√™n t·∫≠p Test Nov-Dec 2024)"))
display(Markdown("="*50 + "\n"))
display(df_evaluation_report.style.format({
    "R-squared": "{:.3f}",
    "MAPE (%)": "{:.2f}%",
    "MAE (ly)": "{:,.1f}",
    "RMSE (ly)": "{:,.1f}"
}).background_gradient(cmap='RdYlGn_r', subset=['MAPE (%)']))


# --- B∆Ø·ªöC 5: TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ ƒê√ÅNH GI√Å (CHI TI·∫æT 4 MODEL) ---
print("\n-- [B∆Ø·ªöC 5] ƒêang tr·ª±c quan h√≥a chi ti·∫øt 4 model (Actual vs Predicted)... --")
fig, axes = plt.subplots(4, 1, figsize=(15, 20), sharex=True)
fig.suptitle('ƒê√ÅNH GI√Å CHI TI·∫æT 4 MODEL (T·∫≠p Test Nov-Dec 2024)', fontsize=16, y=1.02)
model_names = [
    ("Linear Regression", "LR"),
    ("Random Forest (Tuned)", "RF"),
    ("XGBoost (Tuned)", "XGB"),
    ("LSTM", "LSTM")
]
for i, (full_name, short_name) in enumerate(model_names):
    ax = axes[i]
    test_predictions['Actual'].plot(ax=ax, label='S·∫£n l∆∞·ª£ng Th·ª±c t·∫ø (Actual)', style='k-', linewidth=2)
    test_predictions[short_name].plot(ax=ax, label=f'D·ª± b√°o ({full_name})', style='r--', linewidth=2)
    mape_score = df_evaluation_report.loc[full_name]["MAPE (%)"]
    ax.set_title(f'{i+1}. {full_name} (MAPE: {mape_score:.2f}%)')
    ax.set_ylabel('T·ªïng l∆∞·ª£ng b√°n h√†ng ng√†y')
    ax.legend()
    ax.grid(True)
plt.xlabel('Ng√†y')
plt.tight_layout()
plt.savefig('evaluation_chart_grid_4_models.png')
print("Ho√†n t·∫•t. ƒê√£ l∆∞u bi·ªÉu ƒë·ªì l∆∞·ªõi v√†o 'evaluation_chart_grid_4_models.png'.")
plt.show()


# --- GIAI ƒêO·∫†N B: D·ª∞ B√ÅO K·ªäCH B·∫¢N (CHO Q1/2025) ---

print("\n\n" + "="*50)
print(" GIAI ƒêO·∫†N B: D·ª∞ B√ÅO K·ªäCH B·∫¢N (CHO Q1/2025)")
print("="*50)

# --- B∆Ø·ªöC 6: HU·∫§N LUY·ªÜN L·∫†I TR√äN TO√ÄN B·ªò D·ªÆ LI·ªÜU 2024 V√Ä D·ª∞ B√ÅO ---
print("\n-- [B∆Ø·ªöC 6] ƒêang hu·∫•n luy·ªán l·∫°i 4 m√¥ h√¨nh tr√™n TO√ÄN B·ªò d·ªØ li·ªáu 2024... --")

# D·ªØ li·ªáu hu·∫•n luy·ªán (full)
Y_train_full = df_train_2024_full[target_col]
X_train_full = df_train_2024_full[feature_cols]

# D·ªØ li·ªáu t∆∞∆°ng lai (Q1/2025)
df_future_2025 = pd.DataFrame(index=pd.date_range(start='2025-01-01', periods=90, freq='D'))
df_future_2025['Is_Weekend'] = (df_future_2025.index.dayofweek >= 5).astype(int)
df_future_2025['Month'] = df_future_2025.index.month
df_future_2025['DayOfWeek'] = df_future_2025.index.dayofweek
df_future_2025['Is_Holiday'] = 0
df_future_2025['Promotion_Campaign'] = 0

# T·∫°o 2 k·ªãch b·∫£n X_future
X_future_A = df_future_2025.copy()
X_future_A['Price_Index'] = PRICE_INDEX_A
X_future_A = X_future_A[feature_cols]
X_future_B = df_future_2025.copy()
X_future_B['Price_Index'] = PRICE_INDEX_B
X_future_B = X_future_B[feature_cols]

# L∆∞u tr·ªØ k·∫øt qu·∫£
forecast_results = pd.DataFrame(index=df_future_2025.index)
models_summary = {}

# --- 4 Models (Re-training v·ªõi model/tham s·ªë t·ªët nh·∫•t) ---
print("... [1/4] Re-train Linear Regression ...")
model_lr_final = LinearRegression()
model_lr_final.fit(X_train_full, Y_train_full)
pred_A_lr = model_lr_final.predict(X_future_A)
pred_B_lr = model_lr_final.predict(X_future_B)
forecast_results['LR_A'] = pred_A_lr
forecast_results['LR_B'] = pred_B_lr
models_summary['Linear Regression'] = (pred_A_lr.sum(), pred_B_lr.sum())

print("... [2/4] Re-train Random Forest (v·ªõi tham s·ªë T·ªët nh·∫•t)...")
# T·∫°o model RF m·ªõi v·ªõi tham s·ªë t·ªët nh·∫•t t·ª´ Giai ƒëo·∫°n A v√† fit tr√™n 100% data
best_params_rf = random_search_rf.best_params_
model_rf_final = RandomForestRegressor(random_state=42, n_jobs=-1, **best_params_rf)
model_rf_final.fit(X_train_full, Y_train_full) # Fit tr√™n 100% data
pred_A_rf = model_rf_final.predict(X_future_A)
pred_B_rf = model_rf_final.predict(X_future_B)
forecast_results['RF_A'] = pred_A_rf
forecast_results['RF_B'] = pred_B_rf
models_summary['Random Forest (Tuned)'] = (pred_A_rf.sum(), pred_B_rf.sum())

print("... [3/4] Re-train XGBoost (v·ªõi tham s·ªë T·ªët nh·∫•t)...")
# T·∫°o model XGB m·ªõi v·ªõi tham s·ªë t·ªët nh·∫•t v√† fit tr√™n 100% data
best_params_xgb = random_search_xgb.best_params_
model_xgb_final = XGBRegressor(random_state=42, n_jobs=-1, objective='reg:squarederror', **best_params_xgb)
model_xgb_final.fit(X_train_full, Y_train_full) # Fit tr√™n 100% data
pred_A_xgb = model_xgb_final.predict(X_future_A)
pred_B_xgb = model_xgb_final.predict(X_future_B)
forecast_results['XGB_A'] = pred_A_xgb
forecast_results['XGB_B'] = pred_B_xgb
models_summary['XGBoost (Tuned)'] = (pred_A_xgb.sum(), pred_B_xgb.sum())

print("... [4/4] Re-train LSTM ...")
# LSTM c≈©ng ƒë∆∞·ª£c fit l·∫°i tr√™n 100% data
scaler_X_full = MinMaxScaler()
scaler_Y_full = MinMaxScaler()
X_train_scaled_full = scaler_X_full.fit_transform(X_train_full)
Y_train_scaled_full = scaler_Y_full.fit_transform(Y_train_full.values.reshape(-1, 1))
X_train_lstm_full = X_train_scaled_full.reshape((X_train_scaled_full.shape[0], 1, X_train_scaled_full.shape[1]))
# T·∫°o model LSTM m·ªõi
model_lstm_final = Sequential()
model_lstm_final.add(LSTM(50, activation='relu', input_shape=(X_train_lstm_full.shape[1], X_train_lstm_full.shape[2])))
model_lstm_final.add(Dense(1))
model_lstm_final.compile(optimizer='adam', loss='mse')
model_lstm_final.fit(X_train_lstm_full, Y_train_scaled_full, epochs=30, batch_size=32, verbose=0, callbacks=[early_stop]) # TƒÉng epochs

X_future_A_scaled = scaler_X_full.transform(X_future_A)
X_future_B_scaled = scaler_X_full.transform(X_future_B)
X_future_A_lstm = X_future_A_scaled.reshape((X_future_A_scaled.shape[0], 1, X_future_A_scaled.shape[1]))
X_future_B_lstm = X_future_B_scaled.reshape((X_future_B_scaled.shape[0], 1, X_future_B_scaled.shape[1]))
pred_A_lstm_scaled = model_lstm_final.predict(X_future_A_lstm, verbose=0)
pred_B_lstm_scaled = model_lstm_final.predict(X_future_B_lstm, verbose=0)
pred_A_lstm = scaler_Y_full.inverse_transform(pred_A_lstm_scaled).flatten()
pred_B_lstm = scaler_Y_full.inverse_transform(pred_B_lstm_scaled).flatten()
forecast_results['LSTM_A'] = pred_A_lstm
forecast_results['LSTM_B'] = pred_B_lstm
models_summary['LSTM'] = (pred_A_lstm.sum(), pred_B_lstm.sum())

print("\n--- [HO√ÄN T·∫§T] ƒê√£ ch·∫°y xong 4 m√¥ h√¨nh d·ª± b√°o! ---")


# --- B∆Ø·ªöC 7: T·ªîNG H·ª¢P L·ª¢I NHU·∫¨N (Q1/2025) ---
print("\n-- [B∆Ø·ªöC 7] ƒêang t·ªïng h·ª£p k·∫øt qu·∫£ v√† t√≠nh to√°n L·ª£i nhu·∫≠n Q1/2025... --")

df_summary_report = []
for model_name, (total_q_a, total_q_b) in models_summary.items():
    revenue_A = total_q_a * PRICE_PAID_A
    profit_A = total_q_a * (PRICE_PAID_A - NEW_COGS_PER_ITEM)

    revenue_B = total_q_b * PRICE_PAID_B
    profit_B = total_q_b * (PRICE_PAID_B - NEW_COGS_PER_ITEM)

    profit_change = profit_B - profit_A

    df_summary_report.append({
        "Model": model_name,
        "Total_Profit_A (Gi·ªØ gi√°)": profit_A,
        "Total_Profit_B (TƒÉng gi√°)": profit_B,
        "L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m": profit_change,
        "Total_Quantity_A": total_q_a,
        "Total_Quantity_B": total_q_b
    })

df_final_report = pd.DataFrame(df_summary_report).set_index("Model")
df_final_report = df_final_report.sort_values(by="L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m", ascending=False)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
display(Markdown("\n" + "="*50))
display(Markdown("üìä B√ÅO C√ÅO T·ªîNG H·ª¢P D·ª∞ B√ÅO L·ª¢I NHU·∫¨N Q1/2025 (4 M√î H√åNH)"))
display(Markdown(f"D·ª± b√°o cho {N_DAYS_FORECAST} ng√†y (Q1/2025) khi COGS m·ªõi l√† {NEW_COGS_PER_ITEM:,.0f} ƒë/ly."))
display(Markdown("="*50 + "\n"))

format_dict = {
    "Total_Profit_A (Gi·ªØ gi√°)": "{:,.0f} ƒë",
    "Total_Profit_B (TƒÉng gi√°)": "{:,.0f} ƒë",
    "L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m": "{:,.0f} ƒë",
    "Total_Quantity_A": "{:,.0f} ly",
    "Total_Quantity_B": "{:,.0f} ly"
}
display(df_final_report.style.format(format_dict).background_gradient(cmap='RdYlGn', subset=['L·ª£i nhu·∫≠n TƒÉng/Gi·∫£m']))


# --- B∆Ø·ªöC 8: TR·ª∞C QUAN H√ìA K·∫æT QU·∫¢ D·ª∞ B√ÅO (Q1/2025) ---
print("\n-- [B∆Ø·ªöC 8] ƒêang v·∫Ω bi·ªÉu ƒë·ªì d·ª± b√°o (Q1/2025)... --")

best_model_name_forecast = df_final_report.index[0] # L·∫•y model c√≥ l·ª£i nhu·∫≠n tƒÉng/gi·∫£m t·ªët nh·∫•t
prefix_map = {'Linear': 'LR', 'Random': 'RF', 'XGBoost': 'XGB', 'LSTM': 'LSTM'}
best_model_prefix = prefix_map.get(best_model_name_forecast.split(" ")[0], 'XGB')

plt.figure(figsize=(15, 7))
forecast_results[f'{best_model_prefix}_A'].plot(label=f'K·ªãch b·∫£n A: Gi·ªØ gi√° (L∆∞·ª£ng b√°n)', style='b--')
forecast_results[f'{best_model_prefix}_B'].plot(label=f'K·ªãch b·∫£n B: TƒÉng gi√° (L∆∞·ª£ng b√°n)', style='g-')
plt.title(f'D·ª± b√°o L∆∞·ª£ng b√°n Q1/2025 (S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t: {best_model_name_forecast})')
plt.ylabel('T·ªïng l∆∞·ª£ng b√°n h√†ng ng√†y')
plt.xlabel('Ng√†y')
plt.legend()
plt.grid(True)
plt.savefig('forecast_comparison_chart_4_models.png')
print("Ho√†n t·∫•t. ƒê√£ l∆∞u bi·ªÉu ƒë·ªì v√†o 'forecast_comparison_chart_4_models.png'.")
plt.show()